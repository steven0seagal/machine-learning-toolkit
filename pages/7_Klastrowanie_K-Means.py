"""
Klastrowanie K-Means - K-Means Clustering
Educational page with theory and interactive demo
"""

import streamlit as st
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent.parent))
from src.data_loaders import load_breast_cancer_data
from src.plots import plot_elbow_curve, plot_silhouette_scores

st.set_page_config(page_title="K-Means Clustering", page_icon="ğŸ¯", layout="wide")

st.title("ğŸ¯ Klastrowanie K-Means (K-Means Clustering)")

# Create tabs
tab_teoria, tab_demo = st.tabs(["ğŸ“š Teoria i Zastosowania", "ğŸ® Interaktywna Demonstracja"])

with tab_teoria:
    st.header("Teoria i Zastosowania w Bioinformatyce")

    st.markdown("""
    ## 1. Czym jest K-Means Clustering?

    K-Means to **unsupervised learning algorithm** (uczenie nienadzorowane) uÅ¼ywany do
    **klastrowania** - grupowania danych w homogeniczne klastry (grupy) na podstawie
    podobieÅ„stwa.

    ### Kluczowa rÃ³Å¼nica: Supervised vs Unsupervised

    - **Supervised** (Klasyfikacja, Regresja): Mamy etykiety (labels) â†’ model siÄ™ uczy przewidywaÄ‡
    - **Unsupervised** (Klastrowanie): **Nie mamy etykiet** â†’ model znajduje strukturÄ™ w danych

    ### Cel K-Means

    ZnaleÅºÄ‡ **k klastrÃ³w** w danych tak, aby:
    - Punkty **w tym samym klastrze** byÅ‚y jak najbardziej podobne (blisko siebie)
    - Punkty **w rÃ³Å¼nych klastrach** byÅ‚y jak najbardziej rÃ³Å¼ne (daleko od siebie)

    ## 2. Jak dziaÅ‚a algorytm K-Means?

    Algorytm K-Means iteracyjnie przypisuje punkty do klastrÃ³w i aktualizuje centroidy.

    ### Kroki Algorytmu
    """)

    st.latex(r"\text{Minimalizuj: } J = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2")

    st.markdown("""
    Gdzie $C_i$ to klaster $i$, $\\mu_i$ to centroid (Å›rodek) klastra $i$.

    **Algorytm (Lloyd's Algorithm)**:

    1. **Inicjalizacja**: Losowo wybierz k punktÃ³w jako poczÄ…tkowe centroidy

    2. **PrzydziaÅ‚ (Assignment)**:
       - Dla kaÅ¼dego punktu $x$: przypisz do najbliÅ¼szego centroidu
       - UÅ¼ywamy odlegÅ‚oÅ›ci euklidesowej: $d(x, \\mu_i) = ||x - \\mu_i||$

    3. **Aktualizacja (Update)**:
       - Dla kaÅ¼dego klastra: przelicz centroid jako Å›redniÄ… wszystkich punktÃ³w w klastrze
       - $\\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x$

    4. **Powtarzaj** kroki 2-3 aÅ¼:
       - Centroidy przestanÄ… siÄ™ zmieniaÄ‡, LUB
       - OsiÄ…gniÄ™to maksymalnÄ… liczbÄ™ iteracji

    ### Wizualizacja Procesu

    ```
    Iteracja 0: [Losowe centroidy]
    Iteracja 1: Przypisz punkty â†’ Przelicz centroidy
    Iteracja 2: Przypisz punkty â†’ Przelicz centroidy (centroidy siÄ™ przesuwajÄ…)
    ...
    Iteracja N: Przypisz punkty â†’ Centroidy nie zmieniajÄ… siÄ™ â†’ STOP
    ```

    Algorytm **zawsze zbiega** (converges), ale do lokalnego minimum (nie zawsze globalnego!).

    ## 3. WybÃ³r Liczby KlastrÃ³w (k)

    **NajwiÄ™ksze wyzwanie w K-Means**: Ile klastrÃ³w (k) wybraÄ‡?

    Nie ma jednoznacznej odpowiedzi - uÅ¼ywamy heurystyk:

    ### 3.1. Elbow Method (Metoda Åokcia)

    **Idea**: Trenuj K-Means dla rÃ³Å¼nych wartoÅ›ci k (np. 2-10) i rysuj **Inertia** (SSE).
    """)

    st.latex(r"\text{Inertia} = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2")

    st.markdown("""
    - **Inertia** = suma kwadratÃ³w odlegÅ‚oÅ›ci punktÃ³w od ich centroidÃ³w (within-cluster sum of squares)
    - **Im niÅ¼sza Inertia**, tym lepsze dopasowanie (punkty bliÅ¼ej centroidÃ³w)

    **Wykres Inertia vs k**:
    - k=1: Bardzo wysoka Inertia (wszystkie punkty w 1 klastrze)
    - kâ†’âˆ: Inertiaâ†’0 (kaÅ¼dy punkt to osobny klaster)

    **Metoda Åokcia**:
    - Szukaj "Å‚okcia" (elbow) na wykresie
    - Punkt gdzie Inertia zaczyna spadaÄ‡ wolniej
    - To sugerowane **optymalne k**

    ### 3.2. Silhouette Score (WspÃ³Å‚czynnik Sylwetkowy)
    """)

    st.latex(r"s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}")

    st.markdown("""
    Dla kaÅ¼dego punktu $i$:
    - $a(i)$ = Å›rednia odlegÅ‚oÅ›Ä‡ do punktÃ³w w **tym samym** klastrze (compactness)
    - $b(i)$ = Å›rednia odlegÅ‚oÅ›Ä‡ do punktÃ³w w **najbliÅ¼szym innym** klastrze (separation)

    **Silhouette Score**: $s \\in [-1, 1]$
    - **s â‰ˆ 1**: Punkt jest dobrze dopasowany do swojego klastra
    - **s â‰ˆ 0**: Punkt jest na granicy klastrÃ³w
    - **s < 0**: Punkt prawdopodobnie w zÅ‚ym klastrze

    **Average Silhouette Score** (dla wszystkich punktÃ³w):
    - Im wyÅ¼szy (~0.7-1.0), tym lepsze klastrowanie
    - Wybierz **k z najwyÅ¼szym Silhouette Score**

    ### 3.3. Domain Knowledge (Wiedza dziedzinowa)

    Czasami liczba klastrÃ³w jest **znana z gÃ³ry**:
    - Klastrowanie pacjentÃ³w â†’ znamy 3 typy choroby â†’ k=3
    - Klastrowanie genÃ³w â†’ znamy 4 grupy funkcyjne â†’ k=4

    ## 4. Wady i Zalety

    ### âœ… Zalety:

    - **Prostota** - Åatwy do zrozumienia i implementacji
    - **SzybkoÅ›Ä‡** - O(nki) gdzie n=punkty, k=klastry, i=iteracje (zazwyczaj <100)
    - **SkalowalnoÅ›Ä‡** - DziaÅ‚a na duÅ¼ych zbiorach danych
    - **Centroids interpretable** - MoÅ¼na interpretowaÄ‡ centroidy jako "prototypowe" punkty

    ### âŒ Wady:

    - **Trzeba wybraÄ‡ k** - Nie ma automatycznego k
    - **WraÅ¼liwoÅ›Ä‡ na inicjalizacjÄ™** - RÃ³Å¼ne losowe startowe centroidy â†’ rÃ³Å¼ne wyniki
      - RozwiÄ…zanie: `n_init=10` (uruchom 10 razy, wybierz najlepszy)
    - **ZakÅ‚ada sferyczne klastry** - Nie radzi sobie z nieregularnymi ksztaÅ‚tami
    - **WraÅ¼liwoÅ›Ä‡ na outliery** - Outliery mocno wpÅ‚ywajÄ… na centroidy
    - **Wymaga skalowania** - Cechy o duÅ¼ych wartoÅ›ciach dominujÄ… (â†’ StandardScaler!)
    - **Tylko odlegÅ‚oÅ›Ä‡ Euklidesowa** - Nie radzi sobie z danymi kategorycznymi

    ## 5. Zastosowanie w Bioinformatyce: Gene Expression Clustering

    K-Means jest **bardzo popularny** w analizie ekspresji genÃ³w dla odkrywania
    **grup wspÃ³Å‚regulowanych genÃ³w** (co-regulated genes).

    ### Problem

    Eksperyment RNA-seq/mikromacierz:
    - **Wiersze**: Geny (np. 20,000 genÃ³w)
    - **Kolumny**: PrÃ³bki/warunki (np. 10 prÃ³bek)
    - **WartoÅ›ci**: Poziomy ekspresji (FPKM, TPM, log2FC)

    **Pytanie**: KtÃ³re geny zachowujÄ… siÄ™ podobnie w rÃ³Å¼nych warunkach?

    ### Zastosowanie K-Means

    1. **Transpozycja**: Geny jako punkty, warunki jako wymiary (features)
       - KaÅ¼dy gen = punkt w p-wymiarowej przestrzeni (p = liczba warunkÃ³w)

    2. **Normalizacja**:
       - Z-score normalizacja per gen (mean=0, std=1)
       - Lub log2 transformation

    3. **Klastrowanie**:
       - K-Means z k=5-20 (w zaleÅ¼noÅ›ci od danych)
       - KaÅ¼dy klaster = grupa genÃ³w o podobnej ekspresji

    4. **Interpretacja KlastrÃ³w**:
       - **Klaster 1**: Geny upregulated w warunku A (np. stress response)
       - **Klaster 2**: Geny downregulated w warunku B (np. metabolizm)
       - **Klaster 3**: Geny konstytutywne (housekeeping)

    ### Biological Insights
    """)

    st.latex(r"\text{Klaster} \\rightarrow \text{Funkcja Biologiczna (Gene Ontology Enrichment)}")

    st.markdown("""
    **Gene Ontology (GO) Enrichment**:
    - Dla kaÅ¼dego klastra: sprawdÅº jakie funkcje biologiczne sÄ… wzbogacone
    - PrzykÅ‚ad:
      - Klaster 1: Enriched for "DNA repair" â†’ geny odpowiedzi na uszkodzenia DNA
      - Klaster 2: Enriched for "cell cycle" â†’ geny kontroli cyklu komÃ³rkowego

    ### PrzykÅ‚ad: Cancer Subtyping

    K-Means na danych ekspresji genÃ³w pacjentÃ³w z rakiem:
    - **Dane**: 100 pacjentÃ³w Ã— 1000 genÃ³w
    - **Klastrowanie**: K-Means z k=3
    - **Wynik**: 3 podtypy raka (subtypes) z rÃ³Å¼nymi profilami ekspresji
    - **Zastosowanie**: Personalizowana terapia - rÃ³Å¼ne subtypes â†’ rÃ³Å¼ne leki!

    ### Inne Zastosowania w Bioinformatyce

    - **Protein structure clustering** - Grupowanie struktur biaÅ‚ek
    - **Patient stratification** - Segmentacja pacjentÃ³w na grupy
    - **Sequencing read clustering** - Grupowanie readÃ³w DNA/RNA
    - **Metabolomics** - Klastrowanie profili metabolicznych

    ---

    ## ğŸ“– Dodatkowe Zasoby
    - [Scikit-learn K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means)
    - [Gene Expression Clustering](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3184648/)
    - [K-Means in Bioinformatics](https://bmcbioinformatics.biomedcentral.com/)
    """)

with tab_demo:
    st.header("Interaktywna Demonstracja: Breast Cancer Clustering")

    st.markdown("""
    Ten demo pokazuje klastrowanie K-Means na danych Breast Cancer Wisconsin.

    **Uwaga**: To uczenie nienadzorowane - **ignorujemy etykiety** (malignant/benign)
    i patrzymy czy K-Means sam odkryje naturalne grupy w danych!

    Po klastrowaniu **porÃ³wnamy** odkryte klastry z rzeczywistymi etykietami.
    """)

    # Sidebar controls
    st.sidebar.header("âš™ï¸ Ustawienia Demo")

    k = st.sidebar.slider(
        "Liczba klastrÃ³w (k):",
        min_value=2,
        max_value=10,
        value=2,
        step=1
    )

    n_init = st.sidebar.selectbox(
        "Liczba inicjalizacji (n_init):",
        options=[1, 10, 20, 50],
        index=1,
        help="Algorytm uruchomi siÄ™ n_init razy i wybierze najlepszy wynik"
    )

    st.sidebar.markdown("""
    ---
    **WskazÃ³wki:**
    - k=2: SprÃ³buj odtworzyÄ‡ 2 klasy (benign/malignant)
    - k=3-5: Zobacz substruktury w danych
    - ZwiÄ™ksz n_init dla stabilniejszych wynikÃ³w
    """)

    # Load data
    try:
        X, y, feature_names, target_names = load_breast_cancer_data()

        # Standardize features (CRITICAL for K-Means!)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # Apply PCA for 2D visualization
        pca = PCA(n_components=2, random_state=42)
        X_pca = pca.fit_transform(X_scaled)

        # K-Means clustering
        kmeans = KMeans(n_clusters=k, n_init=n_init, random_state=42)
        cluster_labels = kmeans.fit_predict(X_scaled)

        # Metrics
        inertia = kmeans.inertia_
        silhouette = silhouette_score(X_scaled, cluster_labels)

        # Compare with true labels (only for k=2)
        if k == 2:
            from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
            ari = adjusted_rand_score(y, cluster_labels)
            nmi = normalized_mutual_info_score(y, cluster_labels)
        else:
            ari, nmi = None, None

        # Display info
        st.subheader("ğŸ“Š Wyniki Klastrowania")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric(
                "Inertia",
                f"{inertia:.2f}",
                help="Suma kwadratÃ³w odlegÅ‚oÅ›ci - im niÅ¼sza, tym lepiej"
            )
        with col2:
            st.metric(
                "Silhouette Score",
                f"{silhouette:.4f}",
                help="JakoÅ›Ä‡ klastrowania: -1 (Åºle) do 1 (idealnie)"
            )
        with col3:
            if ari is not None:
                st.metric(
                    "ARI vs True Labels",
                    f"{ari:.4f}",
                    help="Adjusted Rand Index: zgodnoÅ›Ä‡ z prawdziwymi etykietami"
                )
            else:
                st.metric("ARI", "N/A", help="DostÄ™pne tylko dla k=2")
        with col4:
            if nmi is not None:
                st.metric(
                    "NMI vs True Labels",
                    f"{nmi:.4f}",
                    help="Normalized Mutual Information"
                )
            else:
                st.metric("NMI", "N/A", help="DostÄ™pne tylko dla k=2")

        # Interpretation
        if silhouette > 0.5:
            st.success(f"""
            âœ… **Dobre klastrowanie!**

            Silhouette Score = {silhouette:.4f} (>0.5) - Klastry sÄ… dobrze rozdzielone i zwarte.
            """)
        elif silhouette > 0.3:
            st.info(f"""
            **Umiarkowane klastrowanie**

            Silhouette Score = {silhouette:.4f} (0.3-0.5) - Klastry sÄ… widoczne, ale mogÄ… siÄ™ nakÅ‚adaÄ‡.
            """)
        else:
            st.warning(f"""
            âš ï¸ **SÅ‚abe klastrowanie**

            Silhouette Score = {silhouette:.4f} (<0.3) - Klastry sÄ… sÅ‚abo rozdzielone.
            MoÅ¼e k jest nieodpowiednie?
            """)

        if k == 2:
            if ari > 0.7:
                st.success(f"""
                âœ… **K-Means odkryÅ‚ prawdziwe klasy!**

                ARI = {ari:.4f} (>0.7) - Klastry K-Means mocno korelujÄ… z prawdziwymi
                etykietami (benign/malignant). To pokazuje, Å¼e dane majÄ… naturalnÄ… strukturÄ™ 2-klasowÄ…!
                """)
            elif ari > 0.4:
                st.info(f"""
                **K-Means czÄ™Å›ciowo odkryÅ‚ klasy**

                ARI = {ari:.4f} (0.4-0.7) - Umiarkowana zgodnoÅ›Ä‡ z prawdziwymi etykietami.
                """)
            else:
                st.warning(f"""
                **K-Means nie odkryÅ‚ klas**

                ARI = {ari:.4f} (<0.4) - SÅ‚aba zgodnoÅ›Ä‡. Klastry K-Means nie odpowiadajÄ…
                prawdziwym etykietom.
                """)

        # Visualization: 2D PCA with clusters
        st.subheader("ğŸ¨ Wizualizacja KlastrÃ³w (PCA 2D)")

        st.markdown(f"""
        Dane sÄ… wysokowymiarowe (30 cech), wiÄ™c uÅ¼ywamy **PCA** do redukcji do 2D dla wizualizacji.

        **Explained Variance**: PC1 + PC2 = {sum(pca.explained_variance_ratio_[:2]):.1%}
        """)

        # Create DataFrame for plotting
        df_plot = pd.DataFrame({
            'PC1': X_pca[:, 0],
            'PC2': X_pca[:, 1],
            'Cluster': cluster_labels.astype(str),
            'True Label': y.map({0: target_names[0], 1: target_names[1]})
        })

        # Plot with cluster colors
        fig_cluster = px.scatter(
            df_plot,
            x='PC1',
            y='PC2',
            color='Cluster',
            title=f'K-Means Clustering (k={k}) - Cluster Labels',
            labels={'PC1': f'PC1 ({pca.explained_variance_ratio_[0]:.1%})',
                    'PC2': f'PC2 ({pca.explained_variance_ratio_[1]:.1%})'},
            hover_data=['True Label'],
            color_discrete_sequence=px.colors.qualitative.Set2
        )

        # Add centroids
        centroids_pca = pca.transform(scaler.transform(
            scaler.inverse_transform(kmeans.cluster_centers_)
        ))
        fig_cluster.add_trace(go.Scatter(
            x=centroids_pca[:, 0],
            y=centroids_pca[:, 1],
            mode='markers',
            marker=dict(size=20, symbol='x', color='black', line=dict(width=2)),
            name='Centroids',
            showlegend=True
        ))

        fig_cluster.update_layout(height=500)
        st.plotly_chart(fig_cluster, use_container_width=True)

        # Plot with true labels for comparison
        if k == 2:
            st.markdown("**PorÃ³wnanie z prawdziwymi etykietami:**")

            fig_true = px.scatter(
                df_plot,
                x='PC1',
                y='PC2',
                color='True Label',
                title='Prawdziwe Etykiety (Benign/Malignant)',
                labels={'PC1': f'PC1 ({pca.explained_variance_ratio_[0]:.1%})',
                        'PC2': f'PC2 ({pca.explained_variance_ratio_[1]:.1%})'},
                color_discrete_map={target_names[0]: 'blue', target_names[1]: 'red'}
            )
            fig_true.update_layout(height=500)
            st.plotly_chart(fig_true, use_container_width=True)

            st.info("""
            **PorÃ³wnaj oba wykresy**:
            - Czy kolory w "Cluster Labels" odpowiadajÄ… kolorom w "True Labels"?
            - JeÅ›li tak â†’ K-Means skutecznie odkryÅ‚ naturalne klasy!
            - JeÅ›li nie â†’ Dane mogÄ… mieÄ‡ bardziej zÅ‚oÅ¼onÄ… strukturÄ™
            """)

        # Elbow Method
        st.subheader("ğŸ“‰ Elbow Method - WybÃ³r Optymalnego k")

        st.markdown("""
        Trenujemy K-Means dla rÃ³Å¼nych wartoÅ›ci k i rysujemy **Inertia vs k**.
        Szukamy "Å‚okcia" na wykresie.
        """)

        with st.spinner('Obliczanie Elbow Plot...'):
            k_range = range(2, 11)
            inertias = []

            for k_test in k_range:
                kmeans_test = KMeans(n_clusters=k_test, n_init=10, random_state=42)
                kmeans_test.fit(X_scaled)
                inertias.append(kmeans_test.inertia_)

            fig_elbow = plot_elbow_curve(inertias, k_range)

            # Highlight current k
            current_k_idx = k - 2 if k >= 2 and k <= 10 else None
            if current_k_idx is not None and current_k_idx < len(inertias):
                fig_elbow.add_trace(go.Scatter(
                    x=[k],
                    y=[inertias[current_k_idx]],
                    mode='markers',
                    marker=dict(size=15, color='red', symbol='star'),
                    name=f'Aktualny k={k}',
                    showlegend=True
                ))

            st.plotly_chart(fig_elbow, use_container_width=True)

            st.markdown("""
            **Interpretacja**:
            - Inertia zawsze maleje gdy k roÅ›nie (wiÄ™cej klastrÃ³w = mniejsze bÅ‚Ä™dy)
            - Szukamy "Å‚okcia" - punktu gdzie krzywa zaczyna wypÅ‚aszczaÄ‡ siÄ™
            - Dla tych danych: Å‚okieÄ‡ czÄ™sto przy **k=2** lub **k=3**
            """)

        # Silhouette Score Plot
        st.subheader("ğŸ“Š Silhouette Score - WybÃ³r Optymalnego k")

        st.markdown("""
        Obliczamy Silhouette Score dla rÃ³Å¼nych wartoÅ›ci k.
        **WyÅ¼szy score = lepsze klastrowanie**.
        """)

        with st.spinner('Obliczanie Silhouette Scores...'):
            silhouette_scores = []

            for k_test in k_range:
                kmeans_test = KMeans(n_clusters=k_test, n_init=10, random_state=42)
                labels_test = kmeans_test.fit_predict(X_scaled)
                score = silhouette_score(X_scaled, labels_test)
                silhouette_scores.append(score)

            fig_silhouette = plot_silhouette_scores(silhouette_scores, k_range)

            # Highlight current k
            if current_k_idx is not None and current_k_idx < len(silhouette_scores):
                fig_silhouette.add_trace(go.Scatter(
                    x=[k],
                    y=[silhouette_scores[current_k_idx]],
                    mode='markers',
                    marker=dict(size=15, color='blue', symbol='diamond'),
                    name=f'Aktualny k={k}',
                    showlegend=True
                ))

            st.plotly_chart(fig_silhouette, use_container_width=True)

            optimal_k = list(k_range)[np.argmax(silhouette_scores)]
            max_silhouette = max(silhouette_scores)

            st.info(f"""
            **Optymalne k wedÅ‚ug Silhouette Score**: **k={optimal_k}** (score={max_silhouette:.4f})

            Czerwona gwiazdka pokazuje k z najwyÅ¼szym Silhouette Score.
            """)

        # Cluster sizes
        with st.expander("ğŸ“Š Rozmiary KlastrÃ³w"):
            cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()

            col1, col2 = st.columns([1, 2])

            with col1:
                st.markdown("**Liczba punktÃ³w w kaÅ¼dym klastrze:**")
                for cluster_id, size in cluster_sizes.items():
                    st.markdown(f"- **Klaster {cluster_id}**: {size} prÃ³bek ({size/len(cluster_labels)*100:.1f}%)")

            with col2:
                fig_sizes = px.bar(
                    x=cluster_sizes.index,
                    y=cluster_sizes.values,
                    labels={'x': 'Klaster', 'y': 'Liczba PrÃ³bek'},
                    title='Rozmiary KlastrÃ³w'
                )
                st.plotly_chart(fig_sizes, use_container_width=True)

        # Experimentation tips
        st.markdown("""
        ---
        ### ğŸ’¡ WskazÃ³wki do eksperymentowania:

        1. **Eksploruj rÃ³Å¼ne k**:
           - k=2: Zobacz czy K-Means odkryje 2 klasy (benign/malignant)
           - k=3-5: MoÅ¼e istniejÄ… subtypy w danych?
           - UÅ¼yj Elbow i Silhouette plots jako wskazÃ³wek

        2. **PorÃ³wnaj z prawdziwymi etykietami**:
           - Dla k=2: SprawdÅº ARI score
           - Czy klastry odpowiadajÄ… benign/malignant?

        3. **Obserwuj Silhouette Score**:
           - >0.7: DoskonaÅ‚e klastrowanie
           - 0.5-0.7: Dobre
           - 0.3-0.5: Umiarkowane
           - <0.3: SÅ‚abe

        4. **StabilnoÅ›Ä‡**:
           - ZmieÅ„ `n_init` na 1 â†’ zobaczysz rÃ³Å¼ne wyniki przy rÃ³Å¼nych uruchomieniach
           - ZwiÄ™ksz do 10-50 â†’ stabilniejsze wyniki

        ### ğŸ§¬ Zastosowanie w Gene Expression:

        W analizie ekspresji genÃ³w, K-Means moÅ¼e odkryÄ‡:
        - **Subtypes nowotworÃ³w** - Pacjenci z podobnymi profilami ekspresji
        - **Co-regulated genes** - Geny ktÃ³re sÄ… razem upregulated/downregulated
        - **Treatment groups** - Pacjenci ktÃ³rzy odpowiadajÄ… podobnie na terapiÄ™

        **PrzykÅ‚ad**: Breast cancer ma subtypes (Luminal A, Luminal B, HER2+, Triple-negative).
        K-Means na danych ekspresji genÃ³w moÅ¼e je odkryÄ‡ **bez etykiet**!

        ### ğŸ” Dlaczego potrzebujemy PCA?

        - Oryginalne dane: 30 wymiarÃ³w (cech)
        - Ludzkie oko: 2-3 wymiary
        - **PCA redukuje 30D â†’ 2D** zachowujÄ…c jak najwiÄ™cej informacji
        - To tylko wizualizacja! K-Means pracuje na oryginalnych 30 cechach
        """)

    except Exception as e:
        st.error(f"BÅ‚Ä…d podczas Å‚adowania danych: {str(e)}")
        st.info("Upewnij siÄ™, Å¼e funkcja load_breast_cancer_data() dziaÅ‚a poprawnie.")
