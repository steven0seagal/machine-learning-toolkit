"""
k-NajbliÅ¼szych SÄ…siadÃ³w (k-NN) - k-Nearest Neighbors
Educational page with theory and interactive demo
"""

import streamlit as st
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score
import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent.parent))
from src.data_loaders import load_breast_cancer_data
from src.plots import plot_decision_boundary_2d

st.set_page_config(page_title="k-NN", page_icon="ğŸ¯", layout="wide")

st.title("ğŸ¯ k-NajbliÅ¼szych SÄ…siadÃ³w (k-NN)")

# Create tabs
tab_teoria, tab_demo = st.tabs(["ğŸ“š Teoria i Zastosowania", "ğŸ® Interaktywna Demonstracja"])

with tab_teoria:
    st.header("Teoria i Zastosowania w Bioinformatyce")

    st.markdown("""
    ## 1. Czym jest k-NajbliÅ¼szych SÄ…siadÃ³w (k-NN)?

    k-NN to jeden z najprostszych algorytmÃ³w uczenia maszynowego. NaleÅ¼y do rodziny
    **"leniwych" algorytmÃ³w** (lazy learners) lub **opartych na instancjach** (instance-based).

    ### Kluczowa Cecha: Brak Fazy Trenowania
    - k-NN **nie buduje** aktywnego modelu podczas treningu
    - Po prostu **zapamiÄ™tuje** caÅ‚y zbiÃ³r treningowy w pamiÄ™ci
    - CaÅ‚e "uczenie" odbywa siÄ™ podczas predykcji!

    ### Jak dziaÅ‚a predykcja?

    Gdy pojawia siÄ™ nowa obserwacja:
    1. **Oblicz odlegÅ‚oÅ›Ä‡** (np. EuklidesowÄ…) do kaÅ¼dego punktu treningowego
    2. **ZnajdÅº k najbliÅ¼szych** sÄ…siadÃ³w
    3. **Klasyfikacja**: GÅ‚osowanie wiÄ™kszoÅ›ciowe (najczÄ™stsza klasa wÅ›rÃ³d k sÄ…siadÃ³w)
    4. **Regresja**: Åšrednia wartoÅ›Ä‡ target z k sÄ…siadÃ³w

    ## 2. Kluczowy Hiperparametr: WybÃ³r k

    To **najwaÅ¼niejszy** hiperparametr. Jego wybÃ³r to balansowanie **kompromisu bias-wariancja**:
    """)

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        ### ğŸ“‰ Niskie k (np. k=1)
        - Bardzo **elastyczny** model
        - **Niski bias**, **wysoka wariancja**
        - WraÅ¼liwy na szum i outliery
        - **Przeuczenie** (overfitting)
        - PostrzÄ™pione granice decyzyjne
        """)

    with col2:
        st.markdown("""
        ### ğŸ“ˆ Wysokie k (np. k=N)
        - Bardzo **sztywny** model
        - **Wysoki bias**, **niska wariancja**
        - Klasyfikuje wszystko do klasy wiÄ™kszoÅ›ciowej
        - **Niedouczenie** (underfitting)
        - GÅ‚adkie granice decyzyjne
        """)

    st.markdown("""
    ### ğŸ’¡ WskazÃ³wki:
    - Dla klasyfikacji binarnej: uÅ¼ywaj **nieparzystego k** (unikniesz remisÃ³w)
    - Typowe wartoÅ›ci: k âˆˆ {3, 5, 7, 9, 11}
    - WybÃ³r k poprzez walidacjÄ™ krzyÅ¼owÄ…

    ## 3. ZaÅ‚oÅ¼enia i Wymagania

    ### âš ï¸ KRYTYCZNE: Skalowanie Cech
    k-NN jest **ekstremalnie wraÅ¼liwy** na skalÄ™ cech:
    - Cechy o duÅ¼ych zakresach (np. 0-10000) dominujÄ… nad maÅ‚ymi (0-1)
    - **ZAWSZE** standaryzuj dane przed uÅ¼yciem k-NN!

    ### Metryka OdlegÅ‚oÅ›ci
    NajczÄ™Å›ciej uÅ¼ywane:
    - **Euklidesowa**: $d = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$
    - **Manhattan**: $d = \\sum_{i=1}^{n} |x_i - y_i|$
    - **Minkowski**: UogÃ³lnienie powyÅ¼szych

    ## 4. Wady i Zalety

    ### âœ… Zalety:
    - **Prostota**: Niezwykle Å‚atwy do implementacji
    - **AdaptowalnoÅ›Ä‡**: Åatwo dodawaÄ‡ nowe dane (bez ponownego treningu)
    - **NieliniowoÅ›Ä‡**: Naturalne obsÅ‚uguje nieliniowe granice
    - **MaÅ‚o hiperparametrÃ³w**: GÅ‚Ã³wnie k i metryka odlegÅ‚oÅ›ci

    ### âŒ Wady:
    - **Koszt obliczeniowy**: Predykcja wymaga porÃ³wnania z KAÅ»DYM punktem treningowym
    - **Nie skaluje siÄ™**: Nie nadaje siÄ™ do duÅ¼ych zbiorÃ³w danych
    - **KlÄ…twa wymiarowoÅ›ci**: DziaÅ‚a sÅ‚abo w wysokowymiarowych przestrzeniach
    - **WraÅ¼liwoÅ›Ä‡ na szum**: SzczegÃ³lnie przy maÅ‚ym k

    ## 5. Zastosowanie w Bioinformatyce: Klasyfikacja Ekspresji GenÃ³w

    Pomimo wad, k-NN jest czÄ™sto uÅ¼ywany w analizie danych z mikromacierzy lub RNA-Seq.

    ### Cel
    Klasyfikacja prÃ³bek biologicznych (np. typÃ³w nowotworÃ³w) na podstawie profilu ekspresji genÃ³w.

    ### Jak to dziaÅ‚a?

    1. **Dane**: KaÅ¼da prÃ³bka (pacjent) = wektor poziomÃ³w ekspresji (~20,000 genÃ³w)
    2. **Metryka**: OdlegÅ‚oÅ›Ä‡ miÄ™dzy prÃ³bkami = rÃ³Å¼nica w profilach ekspresji
    3. **Predykcja**: Nowa prÃ³bka klasyfikowana na podstawie k najbardziej podobnych prÃ³bek

    ### Problem: KlÄ…twa WymiarowoÅ›ci
    - Gdy p (liczba genÃ³w) >> n (liczba prÃ³bek), odlegÅ‚oÅ›ci stajÄ… siÄ™ nieinformatywne
    - **RozwiÄ…zanie**: Selekcja cech lub redukcja wymiaru (PCA) przed k-NN

    ### PrzykÅ‚ad
    - Mamy 100 prÃ³bek nowotworÃ³w (50 Typ A, 50 Typ B)
    - KaÅ¼da prÃ³bka ma ekspresjÄ™ 20,000 genÃ³w
    - UÅ¼ywamy PCA do redukcji do 10 komponentÃ³w
    - k-NN (k=5) klasyfikuje nowÄ… prÃ³bkÄ™ na podstawie 5 najbliÅ¼szych sÄ…siadÃ³w

    ---

    ## ğŸ“– Dodatkowe Zasoby
    - [Scikit-learn k-NN](https://scikit-learn.org/stable/modules/neighbors.html)
    - [k-NN in Bioinformatics](https://academic.oup.com/bioinformatics/)
    """)

with tab_demo:
    st.header("Interaktywna Demonstracja: Breast Cancer Classification")

    st.markdown("""
    Ten demo wizualizuje **kompromis bias-wariancja** w k-NN. Zobaczysz jak wartoÅ›Ä‡ k
    wpÅ‚ywa na ksztaÅ‚t granicy decyzyjnej.

    **Wizualizacja 2D** pokazuje granice decyzyjne (mozaikÄ™ Voronoi) dla rÃ³Å¼nych wartoÅ›ci k.
    """)

    try:
        # Load data
        X, y, feature_names, target_names = load_breast_cancer_data()

        # Sidebar controls
        st.sidebar.header("âš™ï¸ Ustawienia Demo")

        # k parameter
        k = st.sidebar.slider(
            "Liczba sÄ…siadÃ³w (k):",
            min_value=1,
            max_value=51,
            value=5,
            step=2,  # Force odd values
            help="Niskie k = przeuczenie, Wysokie k = niedouczenie"
        )

        st.sidebar.markdown(f"""
        **Wybrane k**: {k}

        - **k=1**: Maksymalne przeuczenie
        - **k=5**: Dobrze zbalansowane
        - **k=51**: Potencjalne niedouczenie
        """)

        # Feature selection for 2D visualization
        default_features = ['mean radius', 'mean texture']
        feature_x = st.sidebar.selectbox(
            "Cecha na osi X:",
            options=list(feature_names),
            index=list(feature_names).index(default_features[0])
        )

        feature_y = st.sidebar.selectbox(
            "Cecha na osi Y:",
            options=list(feature_names),
            index=list(feature_names).index(default_features[1])
        )

        # Get indices
        idx_x = list(feature_names).index(feature_x)
        idx_y = list(feature_names).index(feature_y)

        # Prepare 2D data
        X_2d = X.iloc[:, [idx_x, idx_y]].values
        y_array = y.values

        # CRITICAL: Scale features
        scaler = StandardScaler()
        X_2d_scaled = scaler.fit_transform(X_2d)

        # Train model
        model = KNeighborsClassifier(n_neighbors=k)
        model.fit(X_2d_scaled, y_array)

        # Predictions
        y_pred = model.predict(X_2d_scaled)

        # Metrics
        accuracy = accuracy_score(y_array, y_pred)
        f1 = f1_score(y_array, y_pred)

        # Visualization
        st.subheader("ğŸ“Š Wizualizacja Granicy Decyzyjnej")

        fig = plot_decision_boundary_2d(
            model, X_2d_scaled, y_array,
            [feature_x, feature_y]
        )

        st.plotly_chart(fig, use_container_width=True)

        st.markdown("""
        **Interpretacja:**
        - Mozaika kolorÃ³w pokazuje regiony decyzyjne
        - KaÅ¼dy region naleÅ¼y do jednej klasy
        - Granice sÄ… okreÅ›lone przez najbliÅ¼szych k sÄ…siadÃ³w
        """)

        # Metrics
        st.subheader("ğŸ“ˆ Metryki WydajnoÅ›ci")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("k (liczba sÄ…siadÃ³w)", k)
        with col2:
            st.metric("Accuracy", f"{accuracy:.4f}")
        with col3:
            st.metric("F1-Score", f"{f1:.4f}")

        # Bias-Variance explanation
        st.subheader("âš–ï¸ Kompromis Bias-Wariancja")

        if k <= 3:
            st.warning(f"""
            **k={k}: Wysokie ryzyko przeuczenia!**

            - Granica decyzyjna jest bardzo **postrzÄ™piona**
            - Model idealnie dopasowuje siÄ™ do szumu w danych treningowych
            - **Wysoka wariancja** - maÅ‚e zmiany danych â†’ duÅ¼e zmiany modelu
            - **Niski bias** - model jest bardzo elastyczny
            - MoÅ¼e sÅ‚abo generalizowaÄ‡ na nowe dane
            """)
        elif k >= 25:
            st.info(f"""
            **k={k}: Ryzyko niedouczenia**

            - Granica decyzyjna jest bardzo **gÅ‚adka**
            - Model ignoruje lokalne struktury w danych
            - **Niska wariancja** - model jest stabilny
            - **Wysoki bias** - model jest zbyt sztywny
            - MoÅ¼e nie wychwytywaÄ‡ istotnych wzorcÃ³w
            """)
        else:
            st.success(f"""
            **k={k}: Dobrze zbalansowane!**

            - Granica decyzyjna jest **umiarkowanie zÅ‚oÅ¼ona**
            - Model balansuje dopasowanie i generalizacjÄ™
            - **Åšrednia wariancja i bias**
            - Typowa "dobra" wartoÅ›Ä‡ k dla tego problemu
            """)

        # Experimentation tips
        st.markdown("""
        ---
        ### ğŸ’¡ WskazÃ³wki do eksperymentowania:

        1. **Eksperymentuj z k**:
           - Ustaw k=1 â†’ Zobacz ekstremalnie postrzÄ™pione granice (kaÅ¼dy punkt tworzy wÅ‚asnÄ… wyspÄ™)
           - Ustaw k=5 â†’ Granice sÄ… bardziej gÅ‚adkie, ale wciÄ…Å¼ elastyczne
           - Ustaw k=51 â†’ Bardzo gÅ‚adkie, uogÃ³lnione granice

        2. **Obserwuj metryki**:
           - Przy k=1: Accuracy na danych treningowych bÄ™dzie ~100% (przeuczenie!)
           - Przy optymalnym k: Najlepszy balans
           - Przy duÅ¼ym k: Accuracy spada (niedouczenie)

        3. **ZmieÅ„ cechy**:
           - KtÃ³re pary cech dajÄ… najlepszÄ… separacjÄ™ klas?
           - Czy potrzebujesz wiÄ™kszego czy mniejszego k dla rÃ³Å¼nych cech?

        ### ğŸ”¬ W praktyce:
        - UÅ¼ylibyÅ›my **walidacji krzyÅ¼owej** do wyboru optymalnego k
        - UÅ¼ylibyÅ›my **wszystkich 30 cech**, nie tylko 2
        - Zawsze **standaryzujemy** dane przed k-NN!
        """)

        # Data preview
        with st.expander("ğŸ“‹ Informacje o Danych"):
            st.markdown(f"""
            - **Liczba prÃ³bek**: {len(X)}
            - **Liczba cech**: {len(feature_names)}
            - **Klasy**: {', '.join(target_names)}
            - **RozkÅ‚ad klas**: Benign: {sum(y==1)}, Malignant: {sum(y==0)}
            """)

    except Exception as e:
        st.error(f"BÅ‚Ä…d: {str(e)}")
