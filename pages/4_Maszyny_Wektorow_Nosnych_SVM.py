"""
Maszyny WektorÃ³w NoÅ›nych (SVM) - Support Vector Machines
Educational page with theory and interactive demo
"""

import streamlit as st
import numpy as np
from sklearn.svm import SVC
from sklearn.datasets import make_moons, make_circles
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score
import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent.parent))
from src.plots import plot_decision_boundary_2d
from src.navigation import render_sidebar_navigation

st.set_page_config(page_title="SVM", page_icon="âš›ï¸", layout="wide")

# Render sidebar navigation
render_sidebar_navigation()

st.title("âš›ï¸ Maszyny WektorÃ³w NoÅ›nych (SVM)")

# Create tabs
tab_teoria, tab_demo = st.tabs(["ğŸ“š Teoria i Zastosowania", "ğŸ® Interaktywna Demonstracja"])

with tab_teoria:
    st.header("Teoria i Zastosowania w Bioinformatyce")

    st.markdown("""
    ## 1. Czym sÄ… Maszyny WektorÃ³w NoÅ›nych (SVM)?

    SVM to potÄ™Å¼ny algorytm uczenia nadzorowanego uÅ¼ywany do klasyfikacji, regresji i wykrywania anomalii.
    **SVM jest szczegÃ³lnie efektywny** w przestrzeniach wysokowymiarowych, nawet gdy liczba wymiarÃ³w
    (cech) jest wiÄ™ksza niÅ¼ liczba prÃ³bek!

    ### Podstawowa Idea

    Celem SVM jest znalezienie **optymalnej hiperpÅ‚aszczyzny** (linii w 2D, pÅ‚aszczyzny w 3D),
    ktÃ³ra najlepiej separuje klasy w zbiorze danych.

    **Optymalna hiperpÅ‚aszczyzna** = ta z **maksymalnym marginesem**
    - Margines = odlegÅ‚oÅ›Ä‡ do najbliÅ¼szych punktÃ³w danych z obu klas
    - NajbliÅ¼sze punkty = **wektory noÅ›ne** (support vectors)
    - Tylko wektory noÅ›ne "podtrzymujÄ…" hiperpÅ‚aszczyznÄ™, inne punkty sÄ… ignorowane!

    ## 2. Kernel Trick (Sztuczka JÄ…drowa)

    Wiele rzeczywistych zbiorÃ³w danych **nie jest separowalna liniowo**. SVM radzi sobie z tym
    za pomocÄ… "sztuczki jÄ…drowej"!

    ### Idea
    Dane sÄ… transformowane z oryginalnej przestrzeni (np. 2D) do przestrzeni o wyÅ¼szym wymiarze
    (np. 3D lub nieskoÅ„czonym), gdzie stajÄ… siÄ™ liniowo separowalne.

    ### Funkcje JÄ…drowe (Kernels)
    """)

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        #### 'linear' (Liniowe)
        - Dla danych liniowo separowalnych
        - Najszybsze obliczeniowo
        - Prosta hiperpÅ‚aszczyzna

        #### 'poly' (Wielomianowe)
        - Dla krzywoliniowych granic
        - Parametr: degree (stopieÅ„ wielomianu)
        - Åšrednia zÅ‚oÅ¼onoÅ›Ä‡

        """)

    with col2:
        st.markdown("""
        #### 'rbf' (Radial Basis Function)
        - **Najpopularniejsze!**
        - Dla zÅ‚oÅ¼onych, nieliniowych granic
        - Parametr: gamma
        - Bardzo elastyczne

        #### 'sigmoid'
        - Rzadziej uÅ¼ywane
        - Podobne do sieci neuronowych
        """)

    st.markdown("""
    ## 3. Kluczowe Hiperparametry

    ### C (Parametr Regularyzacji)
    Kontroluje kompromis miÄ™dzy maksymalizacjÄ… marginesu a minimalizacjÄ… bÅ‚Ä™du klasyfikacji.
    """)

    col_c1, col_c2 = st.columns(2)

    with col_c1:
        st.markdown("""
        **Niskie C** (np. 0.01)
        - **MiÄ™kki margines**
        - Toleruje bÅ‚Ä™dy klasyfikacji
        - Szerszy, bardziej ogÃ³lny margines
        - **Niska wariancja, wysoki bias**
        - Ryzyko niedouczenia
        """)

    with col_c2:
        st.markdown("""
        **Wysokie C** (np. 100)
        - **Twardy margines**
        - Stara siÄ™ poprawnie sklasyfikowaÄ‡ kaÅ¼dy punkt
        - WÄ…ski margines
        - **Wysoka wariancja, niski bias**
        - Ryzyko przeuczenia
        """)

    st.markdown("""
    ### gamma (Dla jÄ…dra RBF i Poly)
    Definiuje jak daleko siÄ™ga wpÅ‚yw pojedynczego wektora noÅ›nego.
    """)

    col_g1, col_g2 = st.columns(2)

    with col_g1:
        st.markdown("""
        **Niskie gamma** (np. 0.01)
        - Szeroki wpÅ‚yw
        - GÅ‚adka granica decyzyjna
        - **Niska wariancja, wysoki bias**
        - Model bardziej ogÃ³lny
        """)

    with col_g2:
        st.markdown("""
        **Wysokie gamma** (np. 10)
        - WÄ…ski wpÅ‚yw (tylko najbliÅ¼sze punkty)
        - PofaÅ‚dowana granica decyzyjna
        - **Wysoka wariancja, niski bias**
        - Przeuczenie do pojedynczych punktÃ³w
        """)

    st.markdown("""
    ## 4. Zastosowanie w Bioinformatyce: Klasyfikacja BiaÅ‚ek

    SVM jest jednym z **najskuteczniejszych** algorytmÃ³w w bioinformatyce, szczegÃ³lnie w proteomice.

    ### Cel
    Przewidywanie funkcji, struktury drugorzÄ™dowej, lokalizacji subkomÃ³rkowej lub interakcji biaÅ‚ek
    na podstawie sekwencji aminokwasowej.

    ### Jak to dziaÅ‚a?

    1. **Kodowanie sekwencji**: Sekwencja biaÅ‚kowa (ciÄ…g liter) â†’ wektor liczbowy
       - SkÅ‚ad aminokwasowy
       - PseAAC (Pseudo-amino acid composition)
       - PSSM (Position-Specific Scoring Matrix) - profile ewolucyjne

    2. **PrzestrzeÅ„ wysokowymiarowa**: Typowo p >> n (wiÄ™cej cech niÅ¼ prÃ³bek)
       - 1000+ wymiarÃ³w, 200 prÃ³bek
       - To jest **siÅ‚a SVM**!

    3. **Trenowanie**: SVM (zazwyczaj RBF kernel) na zbiorze biaÅ‚ek o znanej funkcji

    4. **Predykcja**: Klasyfikacja nowych biaÅ‚ek

    ### PrzykÅ‚ady ZastosowaÅ„

    - **Przewidywanie struktury drugorzÄ™dowej**: Î±-helisa, Î²-kartka, pÄ™tla
    - **Lokalizacja subkomÃ³rkowa**: jÄ…dro, mitochondrium, cytoplazma
    - **Funkcja biaÅ‚ka**: enzym, receptor, transporter
    - **Interakcje biaÅ‚ko-biaÅ‚ko**: czy dwa biaÅ‚ka oddziaÅ‚ujÄ…?

    ### Dlaczego SVM?
    - EfektywnoÅ›Ä‡ w przestrzeniach wysokowymiarowych (p > n)
    - OdpornoÅ›Ä‡ na przeuczenie (dziÄ™ki maksymalizacji marginesu)
    - ElastycznoÅ›Ä‡ dziÄ™ki kernelom
    - CzÄ™sto osiÄ…ga najwyÅ¼szÄ… dokÅ‚adnoÅ›Ä‡

    ---

    ## ğŸ“– Dodatkowe Zasoby
    - [Scikit-learn SVM](https://scikit-learn.org/stable/modules/svm.html)
    - [SVM in Bioinformatics](https://bmcbioinformatics.biomedcentral.com/)
    """)

with tab_demo:
    st.header("Interaktywna Demonstracja: Nieliniowe Granice Decyzyjne")

    st.markdown("""
    Ten demo pokazuje **moc jÄ…der nieliniowych** (szczegÃ³lnie RBF) w SVM.
    UÅ¼ywamy syntetycznych danych w ksztaÅ‚cie ksiÄ™Å¼ycÃ³w, ktÃ³re **nie sÄ… liniowo separowalne**.

    **Cel**: Zobaczysz jak kernel i parametry C, gamma wpÅ‚ywajÄ… na granicÄ™ decyzyjnÄ….
    """)

    # Sidebar controls
    st.sidebar.header("âš™ï¸ Ustawienia Demo")

    # Dataset selection
    dataset_type = st.sidebar.selectbox(
        "Typ danych syntetycznych:",
        options=['moons', 'circles'],
        format_func=lambda x: 'KsiÄ™Å¼yce (Moons)' if x == 'moons' else 'KoÅ‚a (Circles)'
    )

    noise_level = st.sidebar.slider(
        "Poziom szumu:",
        min_value=0.0,
        max_value=0.5,
        value=0.3,
        step=0.05
    )

    # Kernel selection
    kernel = st.sidebar.selectbox(
        "Wybierz jÄ…dro (kernel):",
        options=['linear', 'rbf', 'poly'],
        index=1
    )

    # C parameter
    C_exp = st.sidebar.slider(
        "Parametr Regularyzacji (C) - skala log:",
        min_value=-2.0,
        max_value=3.0,
        value=0.0,
        step=0.5
    )
    C = 10 ** C_exp

    # Gamma parameter (only for RBF and poly)
    if kernel in ['rbf', 'poly']:
        gamma_exp = st.sidebar.slider(
            "Parametr Gamma - skala log:",
            min_value=-2.0,
            max_value=2.0,
            value=0.0,
            step=0.5
        )
        gamma = 10 ** gamma_exp
    else:
        gamma = 'scale'

    st.sidebar.markdown(f"""
    ---
    **Aktualne wartoÅ›ci:**
    - C = {C:.3f}
    - Gamma = {gamma if isinstance(gamma, str) else f'{gamma:.3f}'}
    - Kernel = {kernel}
    """)

    # Generate synthetic data
    np.random.seed(42)
    if dataset_type == 'moons':
        X, y = make_moons(n_samples=300, noise=noise_level, random_state=42)
    else:
        X, y = make_circles(n_samples=300, noise=noise_level, factor=0.5, random_state=42)

    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Train SVM
    if kernel in ['rbf', 'poly']:
        model = SVC(kernel=kernel, C=C, gamma=gamma, random_state=42)
    else:
        model = SVC(kernel=kernel, C=C, random_state=42)

    model.fit(X_scaled, y)

    # Predictions
    y_pred = model.predict(X_scaled)

    # Metrics
    accuracy = accuracy_score(y, y_pred)
    f1 = f1_score(y, y_pred)
    n_support = model.n_support_

    # Visualization
    st.subheader("ğŸ“Š Wizualizacja Granicy Decyzyjnej")

    fig = plot_decision_boundary_2d(
        model, X_scaled, y,
        ['Feature 1', 'Feature 2']
    )

    st.plotly_chart(fig, use_container_width=True)

    # Metrics
    st.subheader("ğŸ“ˆ Metryki WydajnoÅ›ci")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Accuracy", f"{accuracy:.4f}")
    with col2:
        st.metric("F1-Score", f"{f1:.4f}")
    with col3:
        st.metric("Support Vectors (Class 0)", n_support[0])
    with col4:
        st.metric("Support Vectors (Class 1)", n_support[1])

    # Kernel explanation
    st.subheader("ğŸ¯ Interpretacja WynikÃ³w")

    if kernel == 'linear':
        st.warning("""
        **Kernel 'linear' - Niepowodzenie!**

        Dane w ksztaÅ‚cie ksiÄ™Å¼ycÃ³w/kÃ³Å‚ **nie sÄ… liniowo separowalne**.
        Liniowa hiperpÅ‚aszczyzna nie moÅ¼e ich poprawnie rozdzieliÄ‡.

        **Accuracy jest niska** (~50-60%), model dziaÅ‚a sÅ‚abo.

        ğŸ’¡ **SprÃ³buj**: PrzeÅ‚Ä…cz siÄ™ na kernel 'rbf'!
        """)
    elif kernel == 'rbf':
        if accuracy > 0.95:
            st.success(f"""
            **Kernel 'rbf' - DoskonaÅ‚e dopasowanie!**

            Accuracy = {accuracy:.2%} - model idealnie separuje klasy!

            Kernel RBF **transformuje dane do przestrzeni wyÅ¼szego wymiaru**,
            gdzie stajÄ… siÄ™ liniowo separowalne.

            **Support Vectors**: {sum(n_support)} punktÃ³w (z {len(X)}) podtrzymuje hiperpÅ‚aszczyznÄ™.
            """)
        elif accuracy < 0.7:
            st.warning(f"""
            **Kernel 'rbf' - Niedouczenie lub przeuczenie**

            Accuracy = {accuracy:.2%} - model nie dziaÅ‚a optymalnie.

            MoÅ¼liwe przyczyny:
            - **C zbyt niskie** â†’ model zbyt prosty (niedouczenie)
            - **Gamma zbyt wysokie/niskie** â†’ nieprawidÅ‚owa skala transformacji

            ğŸ’¡ **SprÃ³buj**: C=1.0, Gamma=1.0
            """)
        else:
            st.info(f"""
            **Kernel 'rbf' - Dobre dopasowanie**

            Accuracy = {accuracy:.2%}

            Model radzi sobie dobrze. MoÅ¼esz sprÃ³bowaÄ‡ dostroiÄ‡ C i gamma
            dla jeszcze lepszych wynikÃ³w.
            """)
    else:  # poly
        st.info("""
        **Kernel 'poly' - Wielomianowa transformacja**

        Kernel wielomianowy moÅ¼e rÃ³wnieÅ¼ modelowaÄ‡ nieliniowe granice,
        ale czÄ™sto RBF dziaÅ‚a lepiej w praktyce.
        """)

    # Experimentation tips
    st.markdown("""
    ---
    ### ğŸ’¡ WskazÃ³wki do eksperymentowania:

    1. **PorÃ³wnaj kernele**:
       - **linear**: Zobacz Å¼e kompletnie zawodzi na tych danych
       - **rbf**: Idealna separacja (przy dobrych parametrach)
       - **poly**: RÃ³wnieÅ¼ moÅ¼e dziaÅ‚aÄ‡, ale RBF czÄ™sto lepsze

    2. **Eksperymentuj z C**:
       - **C=0.01**: Bardzo miÄ™kki margines, moÅ¼e niedouczaÄ‡
       - **C=1.0**: Dobry balans
       - **C=100**: Twardy margines, moÅ¼e przeuczaÄ‡ (granica bardzo postrzÄ™piona)

    3. **Eksperymentuj z gamma** (dla RBF):
       - **gamma=0.01**: Bardzo gÅ‚adka granica (moÅ¼e za prosta)
       - **gamma=1.0**: Umiarkowanie zÅ‚oÅ¼ona (zazwyczaj dobra)
       - **gamma=100**: Absurdalnie pofaÅ‚dowana (przeuczenie!)

    4. **Obserwuj support vectors**:
       - Im wiÄ™cej support vectors, tym bardziej zÅ‚oÅ¼ona granica
       - Idealne modele: niewiele support vectors, wysoka accuracy

    ### ğŸ§¬ Analogia do bioinformatyki:
    Tak jak RBF kernel znajduje nieliniowÄ… granicÄ™ dla ksiÄ™Å¼ycÃ³w,
    tak samo w klasyfikacji biaÅ‚ek SVM znajduje **zÅ‚oÅ¼one wzorce**
    w przestrzeni wysokowymiarowej sekwencji aminokwasowych!
    """)
