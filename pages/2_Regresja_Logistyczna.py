"""
Regresja Logistyczna - Logistic Regression
Educational page with theory and interactive demo
"""

import streamlit as st
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent.parent))
from src.data_loaders import load_breast_cancer_data
from src.plots import plot_probability_boundary_2d, plot_confusion_matrix

st.set_page_config(page_title="Regresja Logistyczna", page_icon="", layout="wide")

st.title(" Regresja Logistyczna (Logistic Regression)")

# Create tabs
tab_teoria, tab_demo = st.tabs([" Teoria i Zastosowania", " Interaktywna Demonstracja"])

with tab_teoria:
    st.header("Teoria i Zastosowania w Bioinformatyce")

    st.markdown("""
    ## 1. Czym jest Regresja Logistyczna?

    Regresja Logistyczna jest fundamentalnym algorytmem uczenia nadzorowanego u偶ywanym do
    **problem贸w klasyfikacyjnych**. Pomimo nazwy, nie su偶y do regresji, lecz do przewidywania
    **prawdopodobiestwa** przynale偶noci do klasy.

    Domylnie u偶ywana jest do **klasyfikacji binarnej** (2 klasy: np. "chory" vs "zdrowy").

    ### Funkcja Logistyczna (Sigmoid)
    Podstaw modelu jest funkcja sigmoid, kt贸ra "ciska" wynik liniowy do zakresu (0, 1):
    """)

    st.latex(r"\sigma(z) = \frac{1}{1 + e^{-z}}")

    st.markdown("""
    Gdzie $z$ jest liniow kombinacj cech:
    """)

    st.latex(r"z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p")

    st.markdown("""
    Wynik $\\sigma(z)$ jest interpretowany jako **prawdopodobiestwo** przynale偶noci do klasy "1".
    Ustalajc pr贸g (zazwyczaj 0.5), model dokonuje klasyfikacji.

    ## 2. Kluczowe Zao偶enia Modelu

    Regresja logistyczna ma mniej rygorystyczne zao偶enia ni偶 liniowa:

    1. **Liniowo Log-Szans** - Liniowa zale偶no midzy X a logarytmem szans (log-odds)
    2. **Brak Multikolinearnoci** - Predyktory nie powinny by silnie skorelowane
    3. **Niezale偶no Obserwacji** - Obserwacje musz by niezale偶ne
    4. **Odpowiednio Du偶a Pr贸ba** - Wymagana wystarczajco du偶a pr贸ba

    ## 3. Miary Ewaluacji (Klasyfikacja)

    Dla modeli klasyfikacyjnych u偶ywamy innych metryk ni偶 w regresji:
    """)

    st.latex(r"\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}")
    st.latex(r"\text{Precision} = \frac{TP}{TP + FP}")
    st.latex(r"\text{Recall (Sensitivity)} = \frac{TP}{TP + FN}")
    st.latex(r"\text{F1-Score} = 2 \times \frac{Precision \times Recall}{Precision + Recall}")

    st.markdown("""
    Gdzie:
    - **TP** (True Positives) - Poprawnie zidentyfikowane przypadki pozytywne
    - **TN** (True Negatives) - Poprawnie zidentyfikowane przypadki negatywne
    - **FP** (False Positives) - Bdnie zidentyfikowane jako pozytywne (Bd typu I)
    - **FN** (False Negatives) - Bdnie zidentyfikowane jako negatywne (Bd typu II)

    ### Metryki:
    - **Accuracy** - Og贸lna dokadno (uwaga: mylca przy niezbalansowanych danych!)
    - **Precision** - Jak bardzo mo偶emy ufa predykcji pozytywnej?
    - **Recall** - Jaki procent faktycznych przypadk贸w pozytywnych wykrylimy?
    - **F1-Score** - rednia harmoniczna Precision i Recall (zbalansowana metryka)

    ## 4. Regularyzacja

    Parametr **C** kontroluje si regularyzacji:
    - **Niskie C** (silna regularyzacja): Prostsza granica, mniej przeuczenia, wy偶szy bias
    - **Wysokie C** (saba regularyzacja): Bardziej zo偶ona granica, wicej przeuczenia, wy偶sza wariancja

    ## 5. Zastosowanie w Genomice: GWAS i SNP

    **GWAS (Genome-Wide Association Studies)** - Badania Asocjacyjne Caego Genomu

    ### Cel
    Identyfikacja wariant贸w genetycznych (SNP - polimorfizm贸w pojedynczego nukleotydu),
    kt贸re s statystycznie powizane z ryzykiem wystpienia choroby.

    ### Jak to dziaa?

    1. **Zbieramy dane**:
       - Grupa "przypadk贸w" (cases) - pacjenci z dan chorob
       - Grupa "kontrolna" (controls) - osoby zdrowe

    2. **Genotypujemy** setki tysicy lub miliony SNP dla ka偶dego osobnika

    3. **Budujemy model**:
       - Zmienna zale偶na ($y$): 1 (case) lub 0 (control)
       - Zmienne niezale偶ne ($X$): genotypy (0, 1, 2 - liczba alleli ryzyka)
         oraz zmienne zak贸cajce (wiek, pe, pochodzenie)

    4. **Model** $P(Choroba | Genotyp)$ pozwala oszacowa:
    """)

    st.latex(r"\text{Odds Ratio} = \frac{P(Choroba|Allel=1)}{P(Choroba|Allel=0)}")

    st.markdown("""
    Informujc nas, o ile dany wariant genetyczny zwiksza lub zmniejsza ryzyko choroby.

    ### Przykad
    SNP rs123456 ma OR = 1.5 dla cukrzycy typu 2, co oznacza, 偶e osoby z allelem ryzyka
    maj 50% wy偶sze ryzyko rozwoju choroby.

    ---

    ##  Dodatkowe Zasoby
    - [Scikit-learn Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
    - [GWAS Overview](https://www.genome.gov/genetics-glossary/Genome-Wide-Association-Studies)
    """)

with tab_demo:
    st.header("Interaktywna Demonstracja: Breast Cancer Classification")

    st.markdown("""
    Ten demo pokazuje wykorzystanie regresji logistycznej do klasyfikacji nowotwor贸w piersi
    jako **zoliwych (Malignant)** lub **agodnych (Benign)** na podstawie cech kom贸rkowych.

    **Wizualizacja 2D** pokazuje granic decyzyjn w przestrzeni dw贸ch wybranych cech.
    """)

    try:
        # Load data
        X, y, feature_names, target_names = load_breast_cancer_data()

        # Sidebar controls
        st.sidebar.header("锔 Ustawienia Demo")

        # Regularization parameter
        C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
        C_labels = ['0.001', '0.01', '0.1', '1.0', '10.0', '100.0', '1000.0']
        C_index = st.sidebar.select_slider(
            "Parametr Regularyzacji C (sia odwrotna)",
            options=range(len(C_values)),
            value=3,
            format_func=lambda x: C_labels[x]
        )
        C = C_values[C_index]

        st.sidebar.markdown(f"""
        **Wybrane C**: {C}

        - **Niskie C**: Silna regularyzacja, prostsza granica
        - **Wysokie C**: Saba regularyzacja, zo偶ona granica
        """)

        # Feature selection for 2D visualization
        default_features = ['mean radius', 'mean texture']
        feature_x = st.sidebar.selectbox(
            "Cecha na osi X:",
            options=list(feature_names),
            index=list(feature_names).index(default_features[0])
        )

        feature_y = st.sidebar.selectbox(
            "Cecha na osi Y:",
            options=list(feature_names),
            index=list(feature_names).index(default_features[1])
        )

        # Get indices of selected features
        idx_x = list(feature_names).index(feature_x)
        idx_y = list(feature_names).index(feature_y)

        # Prepare 2D data
        X_2d = X.iloc[:, [idx_x, idx_y]].values
        y_array = y.values

        # Scale features (CRITICAL for regularization)
        scaler = StandardScaler()
        X_2d_scaled = scaler.fit_transform(X_2d)

        # Train model
        model = LogisticRegression(C=C, random_state=42, max_iter=1000)
        model.fit(X_2d_scaled, y_array)

        # Make predictions
        y_pred = model.predict(X_2d_scaled)

        # Calculate metrics
        accuracy = accuracy_score(y_array, y_pred)
        precision = precision_score(y_array, y_pred)
        recall = recall_score(y_array, y_pred)
        f1 = f1_score(y_array, y_pred)
        cm = confusion_matrix(y_array, y_pred)

        # Visualization
        st.subheader(" Wizualizacja Granicy Decyzyjnej")

        fig = plot_probability_boundary_2d(
            model, X_2d_scaled, y_array,
            target_names, [feature_x, feature_y]
        )

        st.plotly_chart(fig, use_container_width=True)

        st.markdown("""
        **Interpretacja:**
        - Kolor ta pokazuje prawdopodobiestwo przynale偶noci do klasy "Malignant"
        - Czerwone punkty: Malignant (zoliwe)
        - Niebieskie punkty: Benign (agodne)
        - Granica decyzyjna znajduje si w miejscu, gdzie P = 0.5
        """)

        # Metrics
        st.subheader(" Metryki Wydajnoci")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("Accuracy", f"{accuracy:.4f}")
        with col2:
            st.metric("Precision", f"{precision:.4f}")
        with col3:
            st.metric("Recall", f"{recall:.4f}")
        with col4:
            st.metric("F1-Score", f"{f1:.4f}")

        # Confusion Matrix
        st.subheader(" Macierz Pomyek")

        col_cm, col_explain = st.columns([1, 1])

        with col_cm:
            fig_cm = plot_confusion_matrix(cm, ['Benign', 'Malignant'])
            st.plotly_chart(fig_cm, use_container_width=True)

        with col_explain:
            st.markdown(f"""
            **Elementy macierzy:**

            - **True Negatives (TN)**: {cm[0, 0]} - Poprawnie sklasyfikowane jako Benign
            - **False Positives (FP)**: {cm[0, 1]} - Bdnie jako Malignant
            - **False Negatives (FN)**: {cm[1, 0]} - Bdnie jako Benign
            - **True Positives (TP)**: {cm[1, 1]} - Poprawnie jako Malignant

            **Uwaga:** W diagnostyce medycznej FN (przeoczenie raka) jest
            czsto gorszy ni偶 FP (faszywy alarm).
            """)

        # Experimentation tips
        st.markdown("""
        ---
        ###  Wskaz贸wki do eksperymentowania:

        1. **Zmie parametr C**:
           - Ustaw C=0.001 (silna regularyzacja) - granica bdzie prosta
           - Ustaw C=1000 (saba regularyzacja) - granica bdzie zo偶ona
           - Obserwuj wpyw na metryki!

        2. **Zmie cechy**:
           - Wybierz r贸偶ne pary cech
           - Kt贸re pary najlepiej separuj klasy?
           - Czy 'worst' cechy s lepsze ni偶 'mean'?

        3. **Zwr贸 uwag**:
           - Jak regularyzacja wpywa na ksztat granicy?
           - Czy model si przeucz przy wysokim C?
           - Czy wszystkie punkty s poprawnie klasyfikowane?

        ###  Zastosowanie w praktyce:
        W rzeczywistoci u偶ylibymy **wszystkich 30 cech**, a nie tylko 2.
        Wizualizacja 2D su偶y wycznie celom edukacyjnym.
        """)

        # Data preview
        with st.expander(" Podgld Danych (pierwsze 5 wierszy)"):
            preview_df = X.iloc[:5, [idx_x, idx_y]].copy()
            preview_df['target'] = y.iloc[:5].map({0: 'Malignant', 1: 'Benign'})
            st.dataframe(preview_df)

    except Exception as e:
        st.error(f"Bd podczas adowania danych: {str(e)}")
        st.info("Dataset Breast Cancer Wisconsin jest wbudowany w scikit-learn.")
