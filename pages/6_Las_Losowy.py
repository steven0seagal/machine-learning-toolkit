"""
Las Losowy - Random Forest
Educational page with theory and interactive demo
"""

import streamlit as st
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent.parent))
from src.data_loaders import load_breast_cancer_data
from src.plots import plot_confusion_matrix, plot_feature_importance

st.set_page_config(page_title="Las Losowy", page_icon="ğŸŒ²", layout="wide")

st.title("ğŸŒ² Las Losowy (Random Forest)")

# Create tabs
tab_teoria, tab_demo = st.tabs(["ğŸ“š Teoria i Zastosowania", "ğŸ® Interaktywna Demonstracja"])

with tab_teoria:
    st.header("Teoria i Zastosowania w Bioinformatyce")

    st.markdown("""
    ## 1. Czym jest Random Forest?

    Random Forest (Las Losowy) to **ensemble learning algorithm** - metoda, ktÃ³ra Å‚Ä…czy
    predykcje wielu modeli bazowych aby uzyskaÄ‡ lepsze wyniki niÅ¼ pojedynczy model.

    ### Podstawowa Idea

    Random Forest = **Wiele Drzew Decyzyjnych + GÅ‚osowanie**

    Zamiast budowaÄ‡ jedno drzewo decyzyjne, Random Forest buduje **las** setek lub tysiÄ™cy
    drzew, a nastÄ™pnie:
    - **Klasyfikacja**: KaÅ¼de drzewo "gÅ‚osuje" na klasÄ™ â†’ zwracana jest klasa wiÄ™kszoÅ›ciowa
    - **Regresja**: Zwracana jest Å›rednia predykcji wszystkich drzew

    """)

    st.latex(r"\text{Predykcja}_{RF} = \text{majority vote}\{\text{Drzewo}_1, \text{Drzewo}_2, ..., \text{Drzewo}_N\}")

    st.markdown("""
    ### Dlaczego Random Forest jest lepszy niÅ¼ pojedyncze drzewo?

    **Pojedyncze Drzewo Decyzyjne**:
    - **High Variance** - Niestabilne, maÅ‚e zmiany w danych â†’ zupeÅ‚nie inne drzewo
    - **Przeuczenie** - Åatwo dopasowuje siÄ™ do szumu w danych treningowych

    **Random Forest**:
    - **Averaging/Voting** - UÅ›rednianie wielu drzew redukuje wariancjÄ™
    - **Lepsza generalizacja** - Mniej podatny na przeuczenie
    - **Robust** - Stabilny na zmianach w danych

    Koncepcja: "MÄ…droÅ›Ä‡ tÅ‚umu" (Wisdom of Crowds) - wiele niezaleÅ¼nych estymatorÃ³w
    razem podejmujÄ… lepsze decyzje!

    ## 2. Jak dziaÅ‚a Random Forest? - Dwa Å¹rÃ³dÅ‚a LosowoÅ›ci

    Random Forest wprowadza **losowoÅ›Ä‡** podczas trenowania kaÅ¼dego drzewa, aby zapewniÄ‡
    Å¼e drzewa sÄ… **rÃ³Å¼norodne i niezaleÅ¼ne**.

    ### 2.1. Bootstrap Aggregating (Bagging)
    """)

    st.latex(r"\text{Bagging} = \text{Bootstrap} + \text{Aggregating}")

    st.markdown("""
    **Bootstrap**: KaÅ¼de drzewo jest trenowane na **losowej prÃ³bce** danych z powtÃ³rzeniami
    - Mamy N prÃ³bek treningowych
    - Dla kaÅ¼dego drzewa: losuj N prÃ³bek **z powtÃ³rzeniami** (sampling with replacement)
    - RÃ³Å¼ne drzewa widzÄ… trochÄ™ inne dane!

    **PrzykÅ‚ad**: Dane = [1, 2, 3, 4, 5]
    - Drzewo 1 moÅ¼e dostaÄ‡: [1, 2, 2, 4, 5]
    - Drzewo 2 moÅ¼e dostaÄ‡: [1, 3, 3, 3, 5]
    - KaÅ¼da prÃ³bka ma ~63% oryginalnych danych, ~37% duplikatÃ³w

    **Out-of-Bag (OOB) samples**: PrÃ³bki niewykorzystane w trenowaniu danego drzewa (~37%)
    mogÄ… byÄ‡ uÅ¼yte do walidacji!

    ### 2.2. Feature Randomness (LosowoÅ›Ä‡ Cech)

    W kaÅ¼dym wÄ™Åºle kaÅ¼dego drzewa:
    - Zamiast rozwaÅ¼aÄ‡ **wszystkie** cechy do podziaÅ‚u
    - Losujemy **podzbiÃ³r** cech (np. âˆšp dla klasyfikacji, gdzie p = liczba cech)
    - Wybieramy najlepszÄ… cechÄ™ **z tego podzbioru**

    **Dlaczego?** - Decorrelation (dekorelacja drzew)
    - Gdyby wszystkie drzewa widziaÅ‚y wszystkie cechy, mogÅ‚yby byÄ‡ podobne
    - NiektÃ³re cechy mogÄ… dominowaÄ‡ (bardzo informacyjne)
    - Feature randomness wymusza rÃ³Å¼norodnoÅ›Ä‡

    ## 3. Kluczowe Hiperparametry

    ### n_estimators (Liczba Drzew)

    - **Definicja**: Liczba drzew w lesie
    - **Typowe wartoÅ›ci**: 100-500 (wiÄ™cej = lepiej, ale wolniej)
    - **Efekt**:
      - WiÄ™cej drzew â†’ lepsza stabilnoÅ›Ä‡, mniejsza wariancja
      - Po pewnym punkcie (np. 500) zyski sÄ… minimalne
      - **Nigdy nie powoduje przeuczenia!** (ale moÅ¼e zbÄ™dnie spowalniaÄ‡)

    ### max_depth (Maksymalna GÅ‚Ä™bokoÅ›Ä‡)

    - **Definicja**: Maksymalna gÅ‚Ä™bokoÅ›Ä‡ kaÅ¼dego drzewa
    - **Typowe wartoÅ›ci**: None (bez limitu), lub 10-30
    - **Efekt**:
      - None â†’ drzewa rosnÄ… do peÅ‚nej gÅ‚Ä™bokoÅ›ci (powszechne w RF!)
      - Niskie wartoÅ›ci â†’ prostsze drzewa, moÅ¼e niedouczaÄ‡
      - Random Forest czÄ™sto uÅ¼ywa **gÅ‚Ä™bokich drzew** bez problemu przeuczenia

    ### max_features (Liczba Cech w WÄ™Åºle)

    - **Definicja**: Liczba cech do rozwaÅ¼enia przy podziale wÄ™zÅ‚a
    - **Typowe wartoÅ›ci**:
      - 'sqrt' lub 'auto': âˆšp (dla klasyfikacji) - domyÅ›lne
      - 'log2': logâ‚‚(p)
      - Liczba lub procent
    - **Efekt**:
      - Mniej cech â†’ wiÄ™ksza rÃ³Å¼norodnoÅ›Ä‡ drzew (lepsza dekorelacja)
      - WiÄ™cej cech â†’ silniejsze pojedyncze drzewa (ale mogÄ… byÄ‡ podobne)

    ### min_samples_split, min_samples_leaf

    Podobnie jak w pojedynczym drzewie - kontrolujÄ… rozrost drzew.

    ## 4. Wady i Zalety

    ### âœ… Zalety:

    - **Wysoka Accuracy** - CzÄ™sto najlepszy standardowy algorytm ML
    - **OdpornoÅ›Ä‡ na przeuczenie** - DziÄ™ki averaging/voting
    - **Feature Importance** - Automatyczna selekcja cech
    - **Brak wymogu skalowania** - Nie wymaga normalizacji
    - **ObsÅ‚uga Missing Values** - MoÅ¼e radziÄ‡ sobie z brakami (w implementacjach)
    - **Out-of-Bag (OOB) Error** - Darmowa walidacja
    - **Paralelizacja** - Drzewa moÅ¼na trenowaÄ‡ rÃ³wnolegle

    ### âŒ Wady:

    - **Black Box** - Mniej interpretowalny niÅ¼ pojedyncze drzewo (setki drzew!)
    - **Rozmiar modelu** - Setki drzew = duÅ¼y model (pamiÄ™Ä‡, storage)
    - **Czas predykcji** - Wolniejszy niÅ¼ pojedyncze drzewo (musi zapytaÄ‡ wszystkie drzewa)
    - **Gorszy dla regresji na ekstrapolacji** - Nie przewiduje poza zakresem danych

    ## 5. Feature Importance w Random Forest

    Random Forest dostarcza **uÅ›rednione feature importances** z wszystkich drzew!
    """)

    st.latex(r"\text{Importance}_{feature} = \frac{1}{N} \sum_{i=1}^{N} \text{Importance}_{feature, tree_i}")

    st.markdown("""
    - Im czÄ™Å›ciej cecha jest uÅ¼ywana do podziaÅ‚u (i im wiÄ™ksza redukcja Gini/Entropy)
    - Tym wyÅ¼sza importance
    - Bardziej **stabilne** niÅ¼ w pojedynczym drzewie!

    ## 6. Zastosowanie w Bioinformatyce: DTI Prediction

    **DTI (Drug-Target Interaction)** - Przewidywanie czy lek bÄ™dzie oddziaÅ‚ywaÅ‚ z biaÅ‚kiem docelowym.

    ### Problem

    Odkrywanie nowych lekÃ³w jest:
    - **Kosztowne**: 2.6 miliarda USD na 1 lek
    - **CzasochÅ‚onne**: 10-15 lat
    - **Ryzykowne**: 90% kandydatÃ³w na leki odpada

    **RozwiÄ…zanie**: Computational screening - przewidywanie interakcji in silico (na komputerze)
    zamiast testowaÄ‡ wszystko eksperymentalnie!

    ### Jak to dziaÅ‚a?

    **Dane wejÅ›ciowe**:
    1. **Deskryptory leku**:
       - Fingerprint molekularny (np. ECFP4) - 1024-wymiarowy wektor binarny
       - Cechy fizykochemiczne (MW, LogP, HBA, HBD)
       - Struktura 2D/3D

    2. **Deskryptory biaÅ‚ka**:
       - Sekwencja aminokwasowa â†’ deskryptory (pseudo-AAC, CTD)
       - Struktura 3D (jeÅ›li dostÄ™pna)
       - Domeny funkcyjne, motywy

    3. **Target**: Czy lek i biaÅ‚ko oddziaÅ‚ujÄ…? (1 = Tak, 0 = Nie)

    **Model**: Random Forest Classifier
    """)

    st.latex(r"P(\text{Interaction}) = \text{RandomForest}(\text{Drug Features}, \text{Protein Features})")

    st.markdown("""
    **Trenowanie**:
    - Dane: znane pary Drug-Target z baz danych (ChEMBL, DrugBank)
    - Pozytywne: potwierdzone interakcje
    - Negatywne: brak interakcji (ostroÅ¼nie z nieznanymi!)
    - Model: Random Forest z n_estimators=500

    **Predykcja**:
    - Input: nowy lek Ã— znane biaÅ‚ko (lub odwrotnie)
    - Output: PrawdopodobieÅ„stwo interakcji (0-1)
    - Top kandydaci â†’ walidacja eksperymentalna

    ### PrzykÅ‚adowe Wyniki

    Random Forest w DTI prediction osiÄ…ga typowo:
    - **Accuracy**: 85-95%
    - **AUC-ROC**: 0.90-0.98
    - **Przewaga nad**: pojedynczym drzewem, SVM, kNN

    ### Feature Importance â†’ Biological Insights

    Po trenowaniu, feature importances pokazujÄ…:
    - **KtÃ³re cechy leku** sÄ… kluczowe (np. hydrophobicity, aromaticity)
    - **KtÃ³re cechy biaÅ‚ka** sÄ… kluczowe (np. powierzchnia wiÄ…zania, motyw domenowy)
    - **Mechanizmy wiÄ…zania** mogÄ… byÄ‡ wnioskowane!

    ### Inne Zastosowania RF w Bioinformatyce

    - **Klasyfikacja chorÃ³b** - Na podstawie ekspresji genÃ³w, biomarkerÃ³w
    - **Variant calling** - Identyfikacja mutacji z danych sekwencyjnych
    - **Protein function prediction** - Przewidywanie funkcji biaÅ‚ka
    - **microRNA target prediction** - Przewidywanie celÃ³w miRNA

    ---

    ## ğŸ“– Dodatkowe Zasoby
    - [Scikit-learn Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest)
    - [RF in Drug Discovery](https://jcheminf.biomedcentral.com/)
    - [Understanding Random Forests](https://www.stat.berkeley.edu/~breiman/RandomForests/)
    """)

with tab_demo:
    st.header("Interaktywna Demonstracja: Breast Cancer Classification")

    st.markdown("""
    Ten demo pokazuje wykorzystanie Random Forest do klasyfikacji nowotworÃ³w piersi.
    PorÃ³wnaj wyniki z pojedynczym drzewem decyzyjnym (poprzednia strona)!

    **ZbiÃ³r danych**: Breast Cancer Wisconsin Dataset (569 prÃ³bek, 30 cech)
    """)

    # Sidebar controls
    st.sidebar.header("âš™ï¸ Ustawienia Demo")

    n_estimators = st.sidebar.slider(
        "Liczba drzew (n_estimators):",
        min_value=10,
        max_value=500,
        value=100,
        step=10
    )

    max_depth_option = st.sidebar.selectbox(
        "Maksymalna gÅ‚Ä™bokoÅ›Ä‡ (max_depth):",
        options=['None', '5', '10', '20'],
    )
    max_depth = None if max_depth_option == 'None' else int(max_depth_option)

    max_features = st.sidebar.selectbox(
        "Max features na split:",
        options=['sqrt', 'log2', 'None'],
        index=0,
        help="sqrt: âˆšp features, log2: logâ‚‚(p) features, None: wszystkie features"
    )
    max_features = None if max_features == 'None' else max_features

    min_samples_split = st.sidebar.slider(
        "Minimalna liczba prÃ³bek do podziaÅ‚u:",
        min_value=2,
        max_value=50,
        value=2,
        step=1
    )

    test_size = st.sidebar.slider(
        "Rozmiar zbioru testowego (%):",
        min_value=10,
        max_value=50,
        value=20,
        step=5
    ) / 100

    st.sidebar.markdown("""
    ---
    **WskazÃ³wki:**
    - ZwiÄ™ksz `n_estimators` â†’ stabilniejszy model
    - `max_depth=None` â†’ peÅ‚ne drzewa (typowe dla RF)
    - `max_features='sqrt'` â†’ dobra dekorelacja drzew
    """)

    # Load data
    try:
        X, y, feature_names, target_names = load_breast_cancer_data()

        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        # Train Random Forest
        with st.spinner('Trenowanie Random Forest...'):
            model = RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth,
                max_features=max_features,
                min_samples_split=min_samples_split,
                random_state=42,
                n_jobs=-1  # Use all CPU cores
            )
            model.fit(X_train, y_train)

        # Predictions
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        # Metrics
        train_accuracy = accuracy_score(y_train, y_train_pred)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        train_f1 = f1_score(y_train, y_train_pred)
        test_f1 = f1_score(y_test, y_test_pred)

        # Overfitting indicator
        overfitting_gap = train_accuracy - test_accuracy

        # OOB Score (if oob_score was enabled)
        # Note: We'll calculate it separately to demonstrate
        oob_model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            max_features=max_features,
            min_samples_split=min_samples_split,
            oob_score=True,
            random_state=42,
            n_jobs=-1
        )
        oob_model.fit(X_train, y_train)
        oob_score = oob_model.oob_score_

        # Display info
        col1, col2 = st.columns(2)

        with col1:
            st.info(f"""
            **Parametry Modelu:**
            - Liczba drzew: **{n_estimators}**
            - Max gÅ‚Ä™bokoÅ›Ä‡: **{max_depth if max_depth else 'Unlimited'}**
            - Max features: **{max_features if max_features else 'All'}**
            - Min prÃ³bek do podziaÅ‚u: **{min_samples_split}**
            """)

        with col2:
            st.info(f"""
            **PodziaÅ‚ Danych:**
            - Trening: **{len(X_train)} prÃ³bek**
            - Test: **{len(X_test)} prÃ³bek**
            - Klasy: **{target_names[0]} / {target_names[1]}**
            """)

        # Metrics
        st.subheader("ğŸ“ˆ Metryki WydajnoÅ›ci")

        col1, col2, col3, col4, col5 = st.columns(5)

        with col1:
            st.metric(
                "Train Accuracy",
                f"{train_accuracy:.4f}",
                help="DokÅ‚adnoÅ›Ä‡ na zbiorze treningowym"
            )
        with col2:
            st.metric(
                "Test Accuracy",
                f"{test_accuracy:.4f}",
                help="DokÅ‚adnoÅ›Ä‡ na zbiorze testowym"
            )
        with col3:
            st.metric(
                "OOB Score",
                f"{oob_score:.4f}",
                help="Out-of-Bag accuracy (darmowa walidacja!)"
            )
        with col4:
            st.metric(
                "Test F1-Score",
                f"{test_f1:.4f}",
                help="F1-score na zbiorze testowym"
            )
        with col5:
            st.metric(
                "Overfitting Gap",
                f"{overfitting_gap:.4f}",
                delta=f"{-overfitting_gap:.4f}",
                delta_color="inverse",
                help="RÃ³Å¼nica miÄ™dzy Train i Test Accuracy"
            )

        # Performance interpretation
        if test_accuracy > 0.95:
            st.success(f"""
            âœ… **DoskonaÅ‚a wydajnoÅ›Ä‡!**

            Test Accuracy = {test_accuracy:.2%} - Model osiÄ…ga znakomitÄ… accuracy na nowych danych.
            Random Forest skutecznie klasyfikuje nowotwory!

            **OOB Score** ({oob_score:.2%}) jest zbliÅ¼ony do Test Accuracy - dobry znak!
            """)
        elif test_accuracy > 0.90:
            st.success(f"""
            âœ… **Bardzo dobra wydajnoÅ›Ä‡!**

            Test Accuracy = {test_accuracy:.2%} - Model dziaÅ‚a bardzo dobrze.
            """)

        if overfitting_gap < 0.05:
            st.success("""
            âœ… **Model dobrze generalizuje!**

            Niewielka rÃ³Å¼nica miÄ™dzy Train a Test accuracy. Random Forest skutecznie
            redukuje przeuczenie dziÄ™ki ensemble averaging!
            """)
        elif overfitting_gap > 0.15:
            st.warning(f"""
            âš ï¸ **Wykryto przeuczenie**

            Gap = {overfitting_gap:.2%}. MoÅ¼liwe rozwiÄ…zania:
            - ZwiÄ™ksz `min_samples_split`
            - Zmniejsz `max_depth`
            - Zmniejsz `max_features`
            """)

        # Feature Importance
        st.subheader("ğŸ” Feature Importances (WaÅ¼noÅ›Ä‡ Cech)")

        st.markdown(f"""
        **Feature importance** uÅ›redniona z **{n_estimators} drzew** - bardziej stabilna
        niÅ¼ w pojedynczym drzewie!

        Te cechy sÄ… najwaÅ¼niejsze dla modelu Random Forest:
        """)

        # Use the helper function from plots.py
        fig_importance = plot_feature_importance(
            model.feature_importances_,
            feature_names,
            top_n=20
        )

        st.plotly_chart(fig_importance, use_container_width=True)

        # Most important features
        top_5_features = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False).head(5)

        st.markdown("**Top 5 najwaÅ¼niejszych cech:**")
        for idx, row in top_5_features.iterrows():
            st.markdown(f"- **{row['feature']}**: {row['importance']:.4f}")

        st.info("""
        **Biological Interpretation:**

        W klasyfikacji raka piersi, cechy zwiÄ…zane z:
        - **Worst concave points** - wklÄ™sÅ‚oÅ›Ä‡ komÃ³rki (wysoka = zÅ‚oÅ›liwa)
        - **Worst perimeter/area** - duÅ¼e, nieregularne komÃ³rki
        - **Mean concave points** - przeciÄ™tna wklÄ™sÅ‚oÅ›Ä‡

        sÄ… najlepszymi **biomarkerami** do odrÃ³Å¼nienia nowotworÃ³w Å‚agodnych od zÅ‚oÅ›liwych.
        """)

        # Confusion Matrix
        st.subheader("ğŸ“Š Macierz PomyÅ‚ek (Confusion Matrix)")

        col_cm1, col_cm2 = st.columns([1, 1])

        with col_cm1:
            cm = confusion_matrix(y_test, y_test_pred)
            fig_cm = plot_confusion_matrix(cm, target_names)
            st.plotly_chart(fig_cm, use_container_width=True)

        with col_cm2:
            # Calculate detailed metrics
            tn, fp, fn, tp = cm.ravel()

            st.markdown(f"""
            **Interpretacja:**

            - **True Negatives (TN)**: {tn} - PrawidÅ‚owo jako Benign
            - **True Positives (TP)**: {tp} - PrawidÅ‚owo jako Malignant
            - **False Positives (FP)**: {fp} - BÅ‚Ä™dnie jako Malignant
            - **False Negatives (FN)**: {fn} - BÅ‚Ä™dnie jako Benign

            **Sensitivity (Recall)**: {tp/(tp+fn):.2%}
            - Procent zÅ‚oÅ›liwych poprawnie zidentyfikowanych

            **Specificity**: {tn/(tn+fp):.2%}
            - Procent Å‚agodnych poprawnie zidentyfikowanych

            **W diagnostyce medycznej:**
            FN (False Negative) jest **najgorszy** - przegapienie
            raka! Model ma tylko **{fn} FN** - bardzo dobry wynik.
            """)

        # Comparison with Single Tree
        with st.expander("ğŸŒ³ vs ğŸŒ² PorÃ³wnanie: Single Tree vs Random Forest"):
            st.markdown("""
            ### Dlaczego Random Forest jest lepszy?

            WrÃ³Ä‡ do poprzedniej strony (Decision Tree) i porÃ³wnaj wyniki dla tych samych danych:

            **Pojedyncze Drzewo Decyzyjne** (max_depth=5):
            - Test Accuracy: ~93-95%
            - **Wysokie przeuczenie** dla gÅ‚Ä™bokich drzew
            - **Niestabilne** - rÃ³Å¼ne wyniki dla rÃ³Å¼nych splitÃ³w
            - **Interpretowalny** - moÅ¼na zobaczyÄ‡ drzewo

            **Random Forest** (100+ drzew):
            - Test Accuracy: ~95-97% âœ…
            - **Niskie przeuczenie** - lepsze generalizowanie
            - **Stabilny** - consistent wyniki
            - **Mniej interpretowalny** - las setek drzew

            ### Kiedy uÅ¼yÄ‡ czego?

            **UÅ¼yj Decision Tree gdy**:
            - Potrzebujesz **interpretowalnoÅ›ci** (wyjaÅ›niÄ‡ lekarzom/regulatorom)
            - Masz proste dane
            - SzybkoÅ›Ä‡ predykcji jest kluczowa

            **UÅ¼yj Random Forest gdy**:
            - Potrzebujesz **najwyÅ¼szej accuracy**
            - MoÅ¼esz poÅ›wiÄ™ciÄ‡ interpretowalnoÅ›Ä‡
            - Masz zÅ‚oÅ¼one, zaszumione dane
            - Chcesz **feature importance** (stabilniejsze niÅ¼ w drzewie)
            """)

        # Experimentation tips
        st.markdown("""
        ---
        ### ğŸ’¡ WskazÃ³wki do eksperymentowania:

        1. **Liczba Drzew (n_estimators)**:
           - Zacznij od 10: Zobaczysz niestabilne wyniki
           - 100: Typowa wartoÅ›Ä‡ startowa
           - 500: Lepsze, ale wolniejsze
           - Obserwuj: Test accuracy stabilizuje siÄ™ po ~100-200 drzew

        2. **Max Depth**:
           - None (unlimited): Typowe dla RF, dziaÅ‚a dobrze!
           - 5: Bardzo prostsze drzewa
           - PorÃ³wnaj: RF z gÅ‚Ä™bokimi drzewami rzadko przeuczy siÄ™ (dziÄ™ki bagging)

        3. **Max Features**:
           - 'sqrt': DomyÅ›lne, dobra dekorelacja
           - 'log2': Jeszcze wiÄ™ksza dekorelacja (wiÄ™cej rÃ³Å¼norodnoÅ›ci)
           - None (all): Drzewa mogÄ… byÄ‡ bardziej podobne

        4. **OOB Score**:
           - Obserwuj jak OOB Score jest zbliÅ¼ony do Test Accuracy
           - To "darmowa" walidacja - nie potrzeba validation set!

        ### ğŸ§¬ Zastosowanie w Drug Discovery:

        W przewidywaniu Drug-Target Interactions, Random Forest czÄ™sto osiÄ…ga:
        - **95%+ accuracy** na dobrych danych
        - **Feature importance** ujawnia kluczowe cechy leku i biaÅ‚ka
        - **Szybka predykcja** dla miliardÃ³w par Drug-Target (screening)

        Ten model moÅ¼e przesiewaÄ‡ **miliony zwiÄ…zkÃ³w chemicznych** in silico,
        drastycznie redukujÄ…c liczbÄ™ zwiÄ…zkÃ³w do testÃ³w eksperymentalnych!
        """)

        # Data preview
        with st.expander("ğŸ“‹ PodglÄ…d Danych (pierwsze 10 wierszy)"):
            df_display = X.head(10).copy()
            df_display['target'] = y.head(10).map({0: target_names[0], 1: target_names[1]})
            st.dataframe(df_display)

    except Exception as e:
        st.error(f"BÅ‚Ä…d podczas Å‚adowania danych: {str(e)}")
        st.info("Upewnij siÄ™, Å¼e funkcja load_breast_cancer_data() dziaÅ‚a poprawnie.")
