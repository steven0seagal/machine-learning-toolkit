"""
Drzewa Decyzyjne - Decision Trees
Educational page with theory and interactive demo
"""

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent.parent))
from src.data_loaders import load_breast_cancer_data
from src.plots import plot_confusion_matrix
from src.navigation import render_sidebar_navigation

st.set_page_config(page_title="Drzewa Decyzyjne", page_icon="ğŸŒ³", layout="wide")

# Render sidebar navigation
render_sidebar_navigation()

st.title("ğŸŒ³ Drzewa Decyzyjne (Decision Trees)")

# Create tabs
tab_teoria, tab_demo = st.tabs(["ğŸ“š Teoria i Zastosowania", "ğŸ® Interaktywna Demonstracja"])

with tab_teoria:
    st.header("Teoria i Zastosowania w Bioinformatyce")

    st.markdown("""
    ## 1. Czym sÄ… Drzewa Decyzyjne?

    Drzewa decyzyjne to algorytm uczenia maszynowego nadzorowanego, ktÃ³ry modeluje decyzje
    i ich moÅ¼liwe konsekwencje w strukturze drzewa. Algorytm **dzieli przestrzeÅ„ cech**
    na regiony, wykonujÄ…c sekwencjÄ™ decyzji binarnych (tak/nie) w kaÅ¼dym wÄ™Åºle.

    ### Struktura Drzewa

    - **WÄ™zeÅ‚ gÅ‚Ã³wny (Root Node)**: Zawiera wszystkie dane
    - **WÄ™zÅ‚y wewnÄ™trzne (Internal Nodes)**: Testy na cechy (np. "czy wiek > 50?")
    - **LiÅ›cie (Leaf Nodes)**: KoÅ„cowe decyzje (klasy w klasyfikacji)
    - **GaÅ‚Ä™zie (Branches)**: ReprezentujÄ… wynik testu (tak/nie)

    ### Jak dziaÅ‚a budowa drzewa?

    1. **WybÃ³r cechy do podziaÅ‚u**: Algorytm wybiera cechÄ™ i prÃ³g, ktÃ³ry najlepiej dzieli dane
    2. **PodziaÅ‚ danych**: Dane sÄ… dzielone na dwa podzbiory na podstawie testu
    3. **Rekursja**: Proces powtarza siÄ™ dla kaÅ¼dego podzbioru
    4. **Warunek stop**: Proces zatrzymuje siÄ™ gdy:
       - Wszystkie dane w wÄ™Åºle naleÅ¼Ä… do jednej klasy
       - OsiÄ…gniÄ™to maksymalnÄ… gÅ‚Ä™bokoÅ›Ä‡
       - WÄ™zeÅ‚ zawiera zbyt maÅ‚o prÃ³bek

    ## 2. Kryteria PodziaÅ‚u: Gini vs Entropy

    Algorytm musi decydowaÄ‡ **ktÃ³ra cecha i prÃ³g** najlepiej dzielÄ… dane. UÅ¼ywa do tego
    miar "nieczystoÅ›ci" (impurity).

    ### Gini Impurity (Indeks Giniego)
    """)

    st.latex(r"Gini = 1 - \sum_{i=1}^{C} p_i^2")

    st.markdown("""
    Gdzie $p_i$ to proporcja prÃ³bek klasy $i$ w wÄ™Åºle, $C$ to liczba klas.

    - **Gini = 0**: WÄ™zeÅ‚ jest czysty (wszystkie prÃ³bki jednej klasy)
    - **Gini = 0.5**: Maksymalna nieczystoÅ›Ä‡ (dla 2 klas z rÃ³wnymi proporcjami)
    - **Obliczeniowo szybsze** niÅ¼ entropia

    ### Entropy (Entropia Shannona)
    """)

    st.latex(r"Entropy = -\sum_{i=1}^{C} p_i \log_2(p_i)")

    st.markdown("""
    - **Entropy = 0**: WÄ™zeÅ‚ jest czysty
    - **Entropy = 1**: Maksymalna nieczystoÅ›Ä‡ (dla 2 klas z rÃ³wnymi proporcjami)
    - Oparta na teorii informacji
    - **Wolniejsza** niÅ¼ Gini

    ### Gini vs Entropy - co wybraÄ‡?

    - **Praktycznie dajÄ… bardzo podobne wyniki!**
    - **Gini**: DomyÅ›lne w scikit-learn, szybsze
    - **Entropy**: MoÅ¼e tworzyÄ‡ nieco bardziej zrÃ³wnowaÅ¼one drzewa
    - RÃ³Å¼nice sÄ… zazwyczaj minimalne

    ## 3. Wady i Zalety

    ### âœ… Zalety:

    - **InterpretowalnoÅ›Ä‡** - Model "white-box": moÅ¼na wizualnie zobaczyÄ‡ i zrozumieÄ‡ decyzje
    - **Brak wymogu skalowania** - Nie wymaga normalizacji/standaryzacji cech
    - **NieliniowoÅ›Ä‡** - Radzi sobie z nieliniowymi zaleÅ¼noÅ›ciami
    - **ObsÅ‚uga rÃ³Å¼nych typÃ³w danych** - Numeryczne i kategoryczne
    - **OdpornoÅ›Ä‡ na outliery** - Nie wpÅ‚ywajÄ… silnie na podziaÅ‚
    - **Feature importance** - Automatycznie wskazuje waÅ¼ne cechy

    ### âŒ Wady:

    - **Przeuczenie** - SkÅ‚onnoÅ›Ä‡ do budowania zbyt zÅ‚oÅ¼onych drzew (high variance)
    - **NiestabilnoÅ›Ä‡** - MaÅ‚e zmiany w danych mogÄ… prowadziÄ‡ do zupeÅ‚nie innego drzewa
    - **Bias** - Faworyzuje cechy z wieloma wartoÅ›ciami
    - **Granice ortogonalne** - Dzieli przestrzeÅ„ tylko wzdÅ‚uÅ¼ osi (axis-aligned splits)
    - **Gorsze generalizacje** niÅ¼ ensembles (Random Forest, Gradient Boosting)

    ## 4. Kontrola Przeuczenia

    Drzewa decyzyjne Å‚atwo siÄ™ **przeuczajÄ…**, rosnÄ…c gÅ‚Ä™boko aby idealnie dopasowaÄ‡ siÄ™
    do danych treningowych. Kontrolujemy to poprzez:

    ### Parametry Pruning (Przycinania)

    - **max_depth**: Maksymalna gÅ‚Ä™bokoÅ›Ä‡ drzewa (np. 3-10)
    - **min_samples_split**: Minimalna liczba prÃ³bek do podziaÅ‚u wÄ™zÅ‚a (np. 2-20)
    - **min_samples_leaf**: Minimalna liczba prÃ³bek w liÅ›ciu (np. 1-10)
    - **max_leaf_nodes**: Maksymalna liczba liÅ›ci

    **Strategie:**
    - PoczÄ…tek: PozwÃ³l drzewu rosnÄ…Ä‡ gÅ‚Ä™boko â†’ zobaczysz przeuczenie
    - NastÄ™pnie: Ogranicz gÅ‚Ä™bokoÅ›Ä‡ (max_depth=5) â†’ lepsze generalizowanie

    ## 5. Zastosowanie w Bioinformatyce: Selekcja GenÃ³w

    Drzewa decyzyjne sÄ… szeroko stosowane w bioinformatyce, szczegÃ³lnie do **selekcji
    cech (gene selection)** w analizie ekspresji genÃ³w.

    ### Cel
    Identyfikacja **genÃ³w biomarkerowych** - genÃ³w, ktÃ³rych ekspresja najlepiej odrÃ³Å¼nia
    prÃ³bki biologiczne (np. zdrowe vs choroba).

    ### Jak to dziaÅ‚a?

    1. **Dane wejÅ›ciowe**: Macierz ekspresji genÃ³w
       - Wiersze: PrÃ³bki pacjentÃ³w (np. 100 pacjentÃ³w)
       - Kolumny: Geny (np. 20,000 genÃ³w)
       - WartoÅ›ci: Poziomy ekspresji (z RNA-seq, mikromacierzy)

    2. **Trenowanie**: DecisionTreeClassifier(max_depth=5)
       - Klasyfikacja: zdrowy vs chory

    3. **Feature Importance**: Po trenowaniu, drzewo zwraca `feature_importances_`
       - WartoÅ›ci 0-1 dla kaÅ¼dego genu
       - Suma = 1.0
       - **Wysokie wartoÅ›ci = waÅ¼ne geny biomarkerowe**

    4. **Selekcja**: Wybierz top N genÃ³w (np. top 50)

    ### PrzykÅ‚ad
    """)

    st.latex(r"\text{Gene Importance} = \frac{\text{Reduction in Gini/Entropy}}{\text{Total Reduction}}")

    st.markdown("""
    Geny ktÃ³re **najlepiej dzielÄ…** pacjentÃ³w (zdrowi vs chorzy) na wczesnych poziomach
    drzewa majÄ… **najwyÅ¼sze importance**.

    ### Zastosowania

    - **Klasyfikacja typÃ³w nowotworÃ³w** - Identyfikacja genÃ³w biomarkerowych
    - **Diagnoza** - Przewidywanie choroby na podstawie profilu ekspresji
    - **Odkrywanie lekÃ³w** - Identyfikacja genÃ³w docelowych
    - **Medycyna personalizowana** - Stratyfikacja pacjentÃ³w

    ### Dlaczego Drzewa Decyzyjne?

    - **InterpretowalnoÅ›Ä‡**: Lekarze/badacze mogÄ… zobaczyÄ‡ "dlaczego" pacjent zostaÅ‚ sklasyfikowany
    - **Feature Importance**: Automatyczna identyfikacja kluczowych genÃ³w
    - **OdpornoÅ›Ä‡**: Nie wymaga normalizacji, odporne na outliery

    **UWAGA**: W praktyce czÄ™sto uÅ¼ywa siÄ™ **Random Forest** (ensemble drzew) dla lepszej
    accuracy, ale pojedyncze drzewo jest najlepsze dla interpretowalnoÅ›ci!

    ---

    ## ğŸ“– Dodatkowe Zasoby
    - [Scikit-learn Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
    - [Gene Selection with Decision Trees](https://www.ncbi.nlm.nih.gov/pmc/articles/)
    """)

with tab_demo:
    st.header("Interaktywna Demonstracja: Breast Cancer Classification")

    st.markdown("""
    Ten demo pokazuje wykorzystanie drzew decyzyjnych do klasyfikacji nowotworÃ³w piersi
    jako **zÅ‚oÅ›liwe (malignant)** lub **Å‚agodne (benign)** na podstawie cech komÃ³rek.

    **ZbiÃ³r danych**: Breast Cancer Wisconsin Dataset (569 prÃ³bek, 30 cech)
    """)

    # Sidebar controls
    st.sidebar.header("âš™ï¸ Ustawienia Demo")

    criterion = st.sidebar.selectbox(
        "Kryterium podziaÅ‚u:",
        options=['gini', 'entropy'],
        format_func=lambda x: f"{x.capitalize()} Impurity"
    )

    max_depth = st.sidebar.slider(
        "Maksymalna gÅ‚Ä™bokoÅ›Ä‡ drzewa:",
        min_value=1,
        max_value=20,
        value=5,
        step=1
    )

    min_samples_split = st.sidebar.slider(
        "Minimalna liczba prÃ³bek do podziaÅ‚u:",
        min_value=2,
        max_value=50,
        value=2,
        step=1
    )

    test_size = st.sidebar.slider(
        "Rozmiar zbioru testowego (%):",
        min_value=10,
        max_value=50,
        value=20,
        step=5
    ) / 100

    st.sidebar.markdown("""
    ---
    **WskazÃ³wki:**
    - ZwiÄ™ksz `max_depth` â†’ drzewo rosnie gÅ‚Ä™biej
    - Niskie `max_depth` (1-3) â†’ prostsze, bardziej ogÃ³lne
    - Wysokie `max_depth` (>10) â†’ przeuczenie!
    """)

    # Load data
    try:
        X, y, feature_names, target_names = load_breast_cancer_data()

        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        # Train Decision Tree
        model = DecisionTreeClassifier(
            criterion=criterion,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            random_state=42
        )
        model.fit(X_train, y_train)

        # Predictions
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        # Metrics
        train_accuracy = accuracy_score(y_train, y_train_pred)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        train_f1 = f1_score(y_train, y_train_pred)
        test_f1 = f1_score(y_test, y_test_pred)

        # Overfitting indicator
        overfitting_gap = train_accuracy - test_accuracy

        # Display info
        col1, col2 = st.columns(2)

        with col1:
            st.info(f"""
            **Parametry Modelu:**
            - Kryterium: **{criterion}**
            - Max gÅ‚Ä™bokoÅ›Ä‡: **{max_depth}**
            - Min prÃ³bek do podziaÅ‚u: **{min_samples_split}**
            - Liczba liÅ›ci: **{model.get_n_leaves()}**
            - GÅ‚Ä™bokoÅ›Ä‡ drzewa: **{model.get_depth()}**
            """)

        with col2:
            st.info(f"""
            **PodziaÅ‚ Danych:**
            - Trening: **{len(X_train)} prÃ³bek**
            - Test: **{len(X_test)} prÃ³bek**
            - Klasy: **{target_names[0]} / {target_names[1]}**
            """)

        # Metrics
        st.subheader("ğŸ“ˆ Metryki WydajnoÅ›ci")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric(
                "Train Accuracy",
                f"{train_accuracy:.4f}",
                help="DokÅ‚adnoÅ›Ä‡ na zbiorze treningowym"
            )
        with col2:
            st.metric(
                "Test Accuracy",
                f"{test_accuracy:.4f}",
                help="DokÅ‚adnoÅ›Ä‡ na zbiorze testowym"
            )
        with col3:
            st.metric(
                "Test F1-Score",
                f"{test_f1:.4f}",
                help="F1-score na zbiorze testowym"
            )
        with col4:
            st.metric(
                "Overfitting Gap",
                f"{overfitting_gap:.4f}",
                delta=f"{-overfitting_gap:.4f}",
                delta_color="inverse",
                help="RÃ³Å¼nica miÄ™dzy Train i Test Accuracy"
            )

        # Overfitting warning
        if overfitting_gap > 0.1:
            st.warning(f"""
            âš ï¸ **Wykryto przeuczenie!**

            Model ma znacznie wyÅ¼szÄ… accuracy na zbiorze treningowym ({train_accuracy:.2%})
            niÅ¼ testowym ({test_accuracy:.2%}). Gap = {overfitting_gap:.2%}

            **RozwiÄ…zanie**: Zmniejsz `max_depth` lub zwiÄ™ksz `min_samples_split`
            """)
        elif overfitting_gap < 0.02:
            st.success(f"""
            âœ… **Model dobrze generalizuje!**

            Niewielka rÃ³Å¼nica miÄ™dzy accuracy treningowÄ… a testowÄ… ({overfitting_gap:.2%}).
            Model nie jest przeuczony.
            """)

        # Decision Tree Visualization
        st.subheader("ğŸŒ³ Wizualizacja Drzewa Decyzyjnego")

        st.markdown("""
        Drzewo pokazuje **sekwencjÄ™ decyzji** podejmowanych przez model.
        - **Kolor**: Niebieski = Benign (0), PomaraÅ„czowy = Malignant (1)
        - **WartoÅ›Ä‡ Gini/Entropy**: Im niÅ¼sza, tym czystsza klasa w wÄ™Åºle
        """)

        # Plot decision tree
        fig, ax = plt.subplots(figsize=(20, 10))

        # Limit displayed features for readability
        max_displayed_depth = min(max_depth, 4)

        plot_tree(
            model,
            max_depth=max_displayed_depth,
            feature_names=feature_names,
            class_names=target_names,
            filled=True,
            rounded=True,
            fontsize=10,
            ax=ax
        )

        plt.tight_layout()
        st.pyplot(fig)

        if max_depth > 4:
            st.info(f"""
            **Uwaga**: Drzewo ma gÅ‚Ä™bokoÅ›Ä‡ {model.get_depth()}, ale wyÅ›wietlamy tylko
            pierwsze {max_displayed_depth} poziomy dla czytelnoÅ›ci.
            """)

        # Feature Importances
        st.subheader("ğŸ” Feature Importances (WaÅ¼noÅ›Ä‡ Cech)")

        st.markdown("""
        **Feature importance** pokazuje, ktÃ³re cechy byÅ‚y najwaÅ¼niejsze dla modelu.
        WartoÅ›ci sumujÄ… siÄ™ do 1.0.
        """)

        # Get top 15 features
        importances = model.feature_importances_
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False).head(15)

        import plotly.express as px
        fig_importance = px.bar(
            importance_df,
            x='importance',
            y='feature',
            orientation='h',
            title='Top 15 Feature Importances',
            labels={'importance': 'Importance', 'feature': 'Feature'}
        )
        fig_importance.update_layout(height=500, yaxis={'categoryorder': 'total ascending'})

        st.plotly_chart(fig_importance, use_container_width=True)

        # Confusion Matrix
        with st.expander("ğŸ“Š Macierz PomyÅ‚ek (Confusion Matrix)"):
            cm = confusion_matrix(y_test, y_test_pred)
            fig_cm = plot_confusion_matrix(cm, target_names)
            st.plotly_chart(fig_cm, use_container_width=True)

            st.markdown("""
            **Interpretacja Macierzy PomyÅ‚ek:**
            - **True Negatives (TN)**: PrawidÅ‚owo sklasyfikowane jako Benign
            - **True Positives (TP)**: PrawidÅ‚owo sklasyfikowane jako Malignant
            - **False Positives (FP)**: BÅ‚Ä™dnie sklasyfikowane jako Malignant (Type I error)
            - **False Negatives (FN)**: BÅ‚Ä™dnie sklasyfikowane jako Benign (Type II error)

            W diagnostyce medycznej **FN jest gorszy niÅ¼ FP** (lepiej Åºle zdiagnozowaÄ‡
            zdrowego jako chorego, niÅ¼ przegapiÄ‡ chorego)!
            """)

        # Experimentation tips
        st.markdown("""
        ---
        ### ğŸ’¡ WskazÃ³wki do eksperymentowania:

        1. **Przeuczenie vs Niedouczenie**:
           - Ustaw `max_depth=1`: Zobaczysz **niedouczenie** (underfitting) - prosty model
           - Ustaw `max_depth=20`: Zobaczysz **przeuczenie** (overfitting) - Train accuracy â‰ˆ100%
           - ZÅ‚oty Å›rodek: `max_depth=5-7`

        2. **Gini vs Entropy**:
           - PrzeÅ‚Ä…cz miÄ™dzy `gini` i `entropy`
           - ZauwaÅ¼ysz, Å¼e wyniki sÄ… bardzo podobne!

        3. **Feature Importances**:
           - KtÃ³re cechy sÄ… najwaÅ¼niejsze?
           - W tym zbiorze czÄ™sto: `worst concave points`, `worst perimeter`, `mean concave points`
           - To potencjalne **biomarkery** dla diagnozy raka!

        4. **GÅ‚Ä™bokoÅ›Ä‡ Drzewa**:
           - SprawdÅº jak roÅ›nie gap miÄ™dzy Train a Test accuracy gdy zwiÄ™kszasz depth

        ### ğŸ§¬ Biomedyczne wnioski:

        Ten model pokazuje, Å¼e **cechy geometryczne komÃ³rek** (perimeter, area, concavity)
        sÄ… kluczowe dla rozrÃ³Å¼nienia nowotworÃ³w Å‚agodnych i zÅ‚oÅ›liwych.

        W rzeczywistej diagnostyce, model Decision Tree z `max_depth=5` moÅ¼e byÄ‡ uÅ¼ywany jako:
        - **NarzÄ™dzie decyzyjne** dla patologÃ³w
        - **Selektor cech** dla bardziej zÅ‚oÅ¼onych modeli
        - **WstÄ™pny screening** przed biopsjÄ…
        """)

        # Data preview
        with st.expander("ğŸ“‹ PodglÄ…d Danych (pierwsze 10 wierszy)"):
            df_display = X.head(10).copy()
            df_display['target'] = y.head(10).map({0: target_names[0], 1: target_names[1]})
            st.dataframe(df_display)

    except Exception as e:
        st.error(f"BÅ‚Ä…d podczas Å‚adowania danych: {str(e)}")
        st.info("Upewnij siÄ™, Å¼e funkcja load_breast_cancer_data() dziaÅ‚a poprawnie.")
